nohup: 忽略输入
Training llama model 7b using 2 GPUs, 2 batch size per GPU, 32 gradient accumulation steps
Output dir: models/llama2-realtimeqa-7b
[2025-02-17 22:01:33,011] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0217 22:01:34.089000 3424428 site-packages/torch/distributed/run.py:793] 
W0217 22:01:34.089000 3424428 site-packages/torch/distributed/run.py:793] *****************************************
W0217 22:01:34.089000 3424428 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0217 22:01:34.089000 3424428 site-packages/torch/distributed/run.py:793] *****************************************
[2025-02-17 22:01:37,809] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-02-17 22:01:37,863] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-02-17 22:01:38,856] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-02-17 22:01:38,871] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-02-17 22:01:38,871] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
02/17/2025 22:01:38 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True, 'offload_param': {'device': 'cpu'}, 'offload_optimizer': {'device': 'cpu'}}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

[rank0]:[W217 22:01:39.455622751 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
02/17/2025 22:01:39 - INFO - __main__ - Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 2
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True, 'offload_param': {'device': 'cpu'}, 'offload_optimizer': {'device': 'cpu'}}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

[rank1]:[W217 22:01:39.470518987 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
loading configuration file config.json from cache at /home/wangyatong/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json
Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32000
}

loading file tokenizer.model from cache at /home/wangyatong/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.model
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/wangyatong/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/wangyatong/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer_config.json
loading file tokenizer.json from cache at /home/wangyatong/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.json
loading file chat_template.jinja from cache at None
[2025-02-17 22:01:42,886] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
loading weights file model.safetensors from cache at /home/wangyatong/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/model.safetensors.index.json
Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[2025-02-17 22:01:42,889] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

[2025-02-17 22:01:48,526] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:20<00:20, 20.92s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:20<00:20, 20.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 13.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.34s/it]
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 13.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.37s/it]
loading configuration file generation_config.json from cache at /home/wangyatong/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32000. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
02/17/2025 22:02:18 - INFO - __main__ - Initializing LORA model...
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
02/17/2025 22:02:20 - INFO - __main__ - Sample 26227 of the training set: {'input_ids': tensor([    1, 29871,  1724,  ...,   287,   322,  8794]), 'labels': tensor([ -100, 29871,  1724,  ...,   287,   322,  8794]), 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1])}.
02/17/2025 22:02:20 - INFO - __main__ - Sample 15068 of the training set: {'input_ids': tensor([    1, 29871,  9160, 19016,   822,  6270,  1095,   435, 29889, 29967,
        29889,   399,  1131,  9326,   323,  1041,  3250,   393,   540,   674,
         3240,   533,   472,   278,  1095,   310,   278, 29871, 29906, 29900,
        29906, 29906,  4259, 29889,    13, 29908,  3421,  5192,   338, 10423,
          411,  3078,   541,  5360,   322, 20715,  4279,  1699,  7780,   300,
          287,   399,  1131, 29892,  1058,  1497, 16340, 29915, 29879,  6410,
          304,   278,  5373,   617,  1662,   414,   471,   670,  1833,  3926,
        25167,  3271,  3748, 29889,   376,  3112, 29915, 29879,  1063,   385,
         8380, 10657,   322,   263, 15377,  1213,    13, 29909,   694, 29899,
         2634,  4983,   363,   278,  1019,  8914,  6573,   310, 21808, 29892,
          399,  1131,  8022,   630,   714,   310,   278, 12417,  1156, 14171,
          263,   937, 29899, 14486,  5839,   491, 24327,   297, 29871, 29906,
        29900, 29896, 29896, 29889,   512,   670,   937,  5320, 20084, 29892,
          278, 22581,  3234,  5969,  2957, 29871, 29955, 29946, 29889, 29945,
          269, 26514, 29892, 29871, 29896, 29945, 11826,   285,  3774,   793,
        29892,   385, 19578, 29892,   322, 29871, 29906, 29929, 29929, 22002,
          793,  1550,  2326,  1076,  2211,  5282,  6270, 14574,   310,   278,
         8905, 24441, 29889,  3600, 29871, 29955, 29946, 29889, 29945,   269,
        26514,   975,   393, 10638,   310,   931,   526,   278,  1473, 29899,
         3242,  1951, 29871, 29896, 29929, 29947, 29906, 29892,  5742,   871,
         6573,   310,   383,  4183,  2169, 12053,  8037, 29889,    13, 29956,
         1131,   750, 29871, 29896, 29947, 29889, 29945,   901,   269, 26514,
        29892, 29871, 29929, 29900,   901,   660, 29933, 19572,   322, 29871,
        29945, 29896,   901, 22002,   793,   363,  6410,  1135,   738,   916,
         4847,   297,   278, 25167,   297,   263,  3023, 29899, 25682, 10638,
          515, 29871, 29906, 29900, 29896, 29906, 29899, 29906, 29900, 29896,
        29945, 29889,    13, 29956,  1131,   471,   263,  1104,   296,  2222,
         4889,   297, 24327, 29892,   443,  1271,   519,   472,   278,  1298,
          310,  5337,   322,  2221,   304,   767,  8411,  1283,  6270,  6276,
        12398, 29889,  3600,  5839, 29899, 28319,   310, 21828,  9400,   880,
          297,   278, 29871, 29906, 29900, 29896, 29896,  1708, 22450,  9242,
          697,   310,   278, 14176, 13582,   297,  8490,   550,  4955, 29889,
           13,  4178,   670, 19224, 29892,   399,  1131,  2323, 29899,  3179,
        23244,   281, 18217,   287,  3748, 13900,   322,   714,  1266, 20704,
          278,  3748, 29892,   263, 16624,  9215, 12709,   358,   363,   263,
          822,  6270,  6276, 11422, 29889,   399,  1131, 29915, 29879,  3233,
          310,  4688, 29899, 18020,   261,  8022,   749,   471,   443,  4352,
          287, 29889,   940,  1584,  7171,  1089,   287,   341, 18510, 18952,
        29892, 28321,  1473,   363,   278,  9862,   297, 29871, 29906, 29900,
        29896, 29946, 29892,   385, 18677,   364,   279,   537,   363,   822,
         6270, 10769, 29889,    13, 29956,  1131,   674,  3240,   533,   263,
         2211, 29899,  2230,   360, 13152, 29979, 19576,   411,  5320,   937,
        29899, 14318,  2178, 29899,  1184,  4207,   943, 29892,  5320,  1019,
        13432,  3137, 29889,   940,   884,  5331,   278, 25167,   297,   269,
        26514,  8951, 29889,   940, 29915, 29879,   263,   528,  3634, 29899,
          262,   363,   263,  7684, 28015,   300,   297,  5320,  2440,   313,
        29956,  1131,   338,   560,   335,  1821,   363,   278,  6573,   297,
        29871, 29906, 29900, 29906, 29947,   511,   408,   697,   310,  2211,
        10769,   297, 25167,  4955,   304,  5401,   360, 13152, 29979,  2211,
         3064,  1192, 19520, 12537,   322,   319,  5022, 18935, 29889,    13,
        29956,  1131,   338,   278,   871,  4847,  1951, 29871, 29896, 29929,
        29947, 29906,   411, 29871, 29906, 29900, 29899, 11242,   269, 26514,
          297,  2999, 20084,  1192, 29871, 29906, 29900, 29889, 29945,   297,
        29871, 29906, 29900, 29896, 29906,   322, 29871, 29906, 29900, 29896,
        29946, 29889,   940,   338,   697,   310,  2211, 10769,  1951, 29871,
        29896, 29929, 29947, 29906,   411, 29871, 29896, 29900, 29900, 29899,
        11242,   269, 26514,   322,  4832, 29899, 11242,  3001,   323, 29928,
        29879,   297,   670,  6413,   313, 15189,   379,  9800, 21776, 12537,
        29892, 23762,  3938, 22437,   467,   399,  1131,   338,   697,   310,
         1023, 10769,   411,  2999, 13442,   323, 29928, 29879,   322,  5839,
        29899, 28319, 29915, 29879,   297,   670,  6413,   297,   278,  5670,
        27207,  3152,   313, 15189,   379,  9800,   897,   291,  8564,   414,
          467,    13, 29956,  1131,  7450, 29871, 29896, 29900, 29900,  6413,
          269, 26514,   297,   670, 29871, 29896, 29906, 29900,   386,  6413,
         3748,   313, 22606, 29871, 29929, 29892, 29871, 29906, 29900, 29906,
        29900,   511,   278, 11582, 29899, 29888,   809,   342,  8090,   304,
        29871, 29896, 29900, 29900,  6413,   269, 26514,  1951, 29871, 29896,
        29929, 29947, 29906,   313, 21823, 29892,   897, 29924,  5666,   375,
          399,   598,   322, 18885,  7075,   467,    13,   797, 29926, 14886,
         3151,   941,  2356,   670,  5802,  1623,   278, 16116,   310,   670,
         6413, 29892,   408,   540,  5318,   263,  2989, 29899,  2536,   403,
          925,  8951,   297,   278,  4940,  9881, 20084, 29889,  1205,   746,
          540,   471,   373,   278,  1746, 29892,   399,  1131,  1754,   670,
        10122,  7091, 29889,   512,   670,  1473,  4259,   297, 23716, 29892,
          399,  1131,   756, 13126, 29871, 29929, 29889, 29945,   269, 26514,
        29892, 29871, 29941, 29941, 22002,   793,   313, 29896, 29946,   363,
         6410, 29897,   322,  4832, 14517,   822, 21144,   297, 29871, 29896,
        29946,  8090,   310,   825,   674,   367,   670,  2186, 11531, 29889,
           13,  3047,  1023,  8090,  9886,   297,   670, 15983,   653,  6413,
        29892,   399,  1131,   756, 29871, 29896, 29896, 29896, 29889, 29945,
         3001,   269, 26514, 29892, 29871, 29945, 29947, 29900, 22002,   793,
        29892, 29871, 29906, 29955, 11826,   285,  3774,   793, 29892, 29871,
        29953, 29929, 14517,   822, 21144,   322,  1023,  4943, 29899, 25682,
        19578, 29879, 29889,   450, 23716,  9160, 19016, 29915,  2186,  1023,
         8090,   310,   278,  1629,   526,   472,   278, 26484, 12941,  3200,
          322,   278,  3087,  8970, 29871, 29946, 29929,   414, 29889,    13,
         2887, 29811,  1821,   408,   670,   373, 29899,  2671,  4180,   471,
         2645,   670, 29871, 29896, 29906, 29899,  6360,  6413, 29892,   399,
         1131,   471,   925,   408, 10879,  1319,  1283,   278,  1746, 29889,
         3600,   664, 29263,  6909,   363,   278, 24327,  7881,   297,   278,
          281,  1296,   310,   379,  1038, 26655,  3536,  6950,   471,   443,
          862,  3498,   839, 29889,  1152,   670,   664, 29892,   399,  1131,
          471,  4257,   278, 25167, 29915, 29879, 10705, 14617,   880,  2315,
          310,   278,  8905,   297, 29871, 29906, 29900, 29896, 29955, 29892,
          278,  9939, 10657,   297,   278, 25167, 29889,     2]), 'labels': tensor([ -100, 29871,  9160, 19016,   822,  6270,  1095,   435, 29889, 29967,
        29889,   399,  1131,  9326,   323,  1041,  3250,   393,   540,   674,
         3240,   533,   472,   278,  1095,   310,   278, 29871, 29906, 29900,
        29906, 29906,  4259, 29889,    13, 29908,  3421,  5192,   338, 10423,
          411,  3078,   541,  5360,   322, 20715,  4279,  1699,  7780,   300,
          287,   399,  1131, 29892,  1058,  1497, 16340, 29915, 29879,  6410,
          304,   278,  5373,   617,  1662,   414,   471,   670,  1833,  3926,
        25167,  3271,  3748, 29889,   376,  3112, 29915, 29879,  1063,   385,
         8380, 10657,   322,   263, 15377,  1213,    13, 29909,   694, 29899,
         2634,  4983,   363,   278,  1019,  8914,  6573,   310, 21808, 29892,
          399,  1131,  8022,   630,   714,   310,   278, 12417,  1156, 14171,
          263,   937, 29899, 14486,  5839,   491, 24327,   297, 29871, 29906,
        29900, 29896, 29896, 29889,   512,   670,   937,  5320, 20084, 29892,
          278, 22581,  3234,  5969,  2957, 29871, 29955, 29946, 29889, 29945,
          269, 26514, 29892, 29871, 29896, 29945, 11826,   285,  3774,   793,
        29892,   385, 19578, 29892,   322, 29871, 29906, 29929, 29929, 22002,
          793,  1550,  2326,  1076,  2211,  5282,  6270, 14574,   310,   278,
         8905, 24441, 29889,  3600, 29871, 29955, 29946, 29889, 29945,   269,
        26514,   975,   393, 10638,   310,   931,   526,   278,  1473, 29899,
         3242,  1951, 29871, 29896, 29929, 29947, 29906, 29892,  5742,   871,
         6573,   310,   383,  4183,  2169, 12053,  8037, 29889,    13, 29956,
         1131,   750, 29871, 29896, 29947, 29889, 29945,   901,   269, 26514,
        29892, 29871, 29929, 29900,   901,   660, 29933, 19572,   322, 29871,
        29945, 29896,   901, 22002,   793,   363,  6410,  1135,   738,   916,
         4847,   297,   278, 25167,   297,   263,  3023, 29899, 25682, 10638,
          515, 29871, 29906, 29900, 29896, 29906, 29899, 29906, 29900, 29896,
        29945, 29889,    13, 29956,  1131,   471,   263,  1104,   296,  2222,
         4889,   297, 24327, 29892,   443,  1271,   519,   472,   278,  1298,
          310,  5337,   322,  2221,   304,   767,  8411,  1283,  6270,  6276,
        12398, 29889,  3600,  5839, 29899, 28319,   310, 21828,  9400,   880,
          297,   278, 29871, 29906, 29900, 29896, 29896,  1708, 22450,  9242,
          697,   310,   278, 14176, 13582,   297,  8490,   550,  4955, 29889,
           13,  4178,   670, 19224, 29892,   399,  1131,  2323, 29899,  3179,
        23244,   281, 18217,   287,  3748, 13900,   322,   714,  1266, 20704,
          278,  3748, 29892,   263, 16624,  9215, 12709,   358,   363,   263,
          822,  6270,  6276, 11422, 29889,   399,  1131, 29915, 29879,  3233,
          310,  4688, 29899, 18020,   261,  8022,   749,   471,   443,  4352,
          287, 29889,   940,  1584,  7171,  1089,   287,   341, 18510, 18952,
        29892, 28321,  1473,   363,   278,  9862,   297, 29871, 29906, 29900,
        29896, 29946, 29892,   385, 18677,   364,   279,   537,   363,   822,
         6270, 10769, 29889,    13, 29956,  1131,   674,  3240,   533,   263,
         2211, 29899,  2230,   360, 13152, 29979, 19576,   411,  5320,   937,
        29899, 14318,  2178, 29899,  1184,  4207,   943, 29892,  5320,  1019,
        13432,  3137, 29889,   940,   884,  5331,   278, 25167,   297,   269,
        26514,  8951, 29889,   940, 29915, 29879,   263,   528,  3634, 29899,
          262,   363,   263,  7684, 28015,   300,   297,  5320,  2440,   313,
        29956,  1131,   338,   560,   335,  1821,   363,   278,  6573,   297,
        29871, 29906, 29900, 29906, 29947,   511,   408,   697,   310,  2211,
        10769,   297, 25167,  4955,   304,  5401,   360, 13152, 29979,  2211,
         3064,  1192, 19520, 12537,   322,   319,  5022, 18935, 29889,    13,
        29956,  1131,   338,   278,   871,  4847,  1951, 29871, 29896, 29929,
        29947, 29906,   411, 29871, 29906, 29900, 29899, 11242,   269, 26514,
          297,  2999, 20084,  1192, 29871, 29906, 29900, 29889, 29945,   297,
        29871, 29906, 29900, 29896, 29906,   322, 29871, 29906, 29900, 29896,
        29946, 29889,   940,   338,   697,   310,  2211, 10769,  1951, 29871,
        29896, 29929, 29947, 29906,   411, 29871, 29896, 29900, 29900, 29899,
        11242,   269, 26514,   322,  4832, 29899, 11242,  3001,   323, 29928,
        29879,   297,   670,  6413,   313, 15189,   379,  9800, 21776, 12537,
        29892, 23762,  3938, 22437,   467,   399,  1131,   338,   697,   310,
         1023, 10769,   411,  2999, 13442,   323, 29928, 29879,   322,  5839,
        29899, 28319, 29915, 29879,   297,   670,  6413,   297,   278,  5670,
        27207,  3152,   313, 15189,   379,  9800,   897,   291,  8564,   414,
          467,    13, 29956,  1131,  7450, 29871, 29896, 29900, 29900,  6413,
          269, 26514,   297,   670, 29871, 29896, 29906, 29900,   386,  6413,
         3748,   313, 22606, 29871, 29929, 29892, 29871, 29906, 29900, 29906,
        29900,   511,   278, 11582, 29899, 29888,   809,   342,  8090,   304,
        29871, 29896, 29900, 29900,  6413,   269, 26514,  1951, 29871, 29896,
        29929, 29947, 29906,   313, 21823, 29892,   897, 29924,  5666,   375,
          399,   598,   322, 18885,  7075,   467,    13,   797, 29926, 14886,
         3151,   941,  2356,   670,  5802,  1623,   278, 16116,   310,   670,
         6413, 29892,   408,   540,  5318,   263,  2989, 29899,  2536,   403,
          925,  8951,   297,   278,  4940,  9881, 20084, 29889,  1205,   746,
          540,   471,   373,   278,  1746, 29892,   399,  1131,  1754,   670,
        10122,  7091, 29889,   512,   670,  1473,  4259,   297, 23716, 29892,
          399,  1131,   756, 13126, 29871, 29929, 29889, 29945,   269, 26514,
        29892, 29871, 29941, 29941, 22002,   793,   313, 29896, 29946,   363,
         6410, 29897,   322,  4832, 14517,   822, 21144,   297, 29871, 29896,
        29946,  8090,   310,   825,   674,   367,   670,  2186, 11531, 29889,
           13,  3047,  1023,  8090,  9886,   297,   670, 15983,   653,  6413,
        29892,   399,  1131,   756, 29871, 29896, 29896, 29896, 29889, 29945,
         3001,   269, 26514, 29892, 29871, 29945, 29947, 29900, 22002,   793,
        29892, 29871, 29906, 29955, 11826,   285,  3774,   793, 29892, 29871,
        29953, 29929, 14517,   822, 21144,   322,  1023,  4943, 29899, 25682,
        19578, 29879, 29889,   450, 23716,  9160, 19016, 29915,  2186,  1023,
         8090,   310,   278,  1629,   526,   472,   278, 26484, 12941,  3200,
          322,   278,  3087,  8970, 29871, 29946, 29929,   414, 29889,    13,
         2887, 29811,  1821,   408,   670,   373, 29899,  2671,  4180,   471,
         2645,   670, 29871, 29896, 29906, 29899,  6360,  6413, 29892,   399,
         1131,   471,   925,   408, 10879,  1319,  1283,   278,  1746, 29889,
         3600,   664, 29263,  6909,   363,   278, 24327,  7881,   297,   278,
          281,  1296,   310,   379,  1038, 26655,  3536,  6950,   471,   443,
          862,  3498,   839, 29889,  1152,   670,   664, 29892,   399,  1131,
          471,  4257,   278, 25167, 29915, 29879, 10705, 14617,   880,  2315,
          310,   278,  8905,   297, 29871, 29906, 29900, 29896, 29955, 29892,
          278,  9939, 10657,   297,   278, 25167, 29889,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.
02/17/2025 22:02:20 - INFO - __main__ - Sample 23320 of the training set: {'input_ids': tensor([    1, 29871,   349,  ...,   263,   716, 29871]), 'labels': tensor([ -100, 29871,   349,  ...,   263,   716, 29871]), 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1])}.
Using /home/wangyatong/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /home/wangyatong/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3712339401245117 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-02-17 22:02:23,724] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.3, git-hash=unknown, git-branch=unknown
[2025-02-17 22:02:23,724] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
Using /home/wangyatong/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /home/wangyatong/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.384183168411255 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-02-17 22:02:23,756] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
[2025-02-17 22:02:23,769] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-02-17 22:02:23,772] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-02-17 22:02:23,773] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-02-17 22:02:23,849] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2025-02-17 22:02:23,849] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2025-02-17 22:02:23,849] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2025-02-17 22:02:23,849] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2025-02-17 22:02:24,026] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2025-02-17 22:02:24,026] [INFO] [utils.py:782:see_memory_usage] MA 0.79 GB         Max_MA 1.08 GB         CA 1.32 GB         Max_CA 1 GB 
[2025-02-17 22:02:24,027] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 41.87 GB, percent = 11.1%
[2025-02-17 22:02:24,035] [INFO] [stage3.py:169:__init__] Reduce bucket size 16777216
[2025-02-17 22:02:24,035] [INFO] [stage3.py:170:__init__] Prefetch bucket size 15099494
[2025-02-17 22:02:24,207] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-02-17 22:02:24,207] [INFO] [utils.py:782:see_memory_usage] MA 0.79 GB         Max_MA 0.79 GB         CA 1.32 GB         Max_CA 1 GB 
[2025-02-17 22:02:24,208] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 41.87 GB, percent = 11.1%
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2025-02-17 22:02:24,913] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-02-17 22:02:24,914] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.79 GB         CA 1.32 GB         Max_CA 1 GB 
[2025-02-17 22:02:24,914] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 42.15 GB, percent = 11.2%
[2025-02-17 22:02:25,087] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2025-02-17 22:02:25,088] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 1.32 GB         Max_CA 1 GB 
[2025-02-17 22:02:25,088] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 42.15 GB, percent = 11.2%
[2025-02-17 22:02:25,468] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1
[2025-02-17 22:02:25,469] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 1.32 GB         Max_CA 1 GB 
[2025-02-17 22:02:25,469] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 43.07 GB, percent = 11.4%
[2025-02-17 22:02:25,657] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2025-02-17 22:02:25,657] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 1.32 GB         Max_CA 1 GB 
[2025-02-17 22:02:25,657] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 42.96 GB, percent = 11.4%
[2025-02-17 22:02:26,047] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2025-02-17 22:02:26,048] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 1.32 GB         Max_CA 1 GB 
[2025-02-17 22:02:26,048] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 43.26 GB, percent = 11.5%
[2025-02-17 22:02:26,236] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-02-17 22:02:26,237] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 1.32 GB         Max_CA 1 GB 
[2025-02-17 22:02:26,237] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 43.71 GB, percent = 11.6%
[2025-02-17 22:02:26,685] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-02-17 22:02:26,686] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 1.32 GB         Max_CA 1 GB 
[2025-02-17 22:02:26,686] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 43.86 GB, percent = 11.6%
[2025-02-17 22:02:26,687] [INFO] [stage3.py:529:_setup_for_real_optimizer] optimizer state initialized
[2025-02-17 22:02:27,139] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-02-17 22:02:27,140] [INFO] [utils.py:782:see_memory_usage] MA 0.03 GB         Max_MA 0.03 GB         CA 1.32 GB         Max_CA 1 GB 
[2025-02-17 22:02:27,140] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 44.15 GB, percent = 11.7%
[2025-02-17 22:02:27,141] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2025-02-17 22:02:27,143] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-02-17 22:02:27,144] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-02-17 22:02:27,144] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2025-02-17 22:02:27,151] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2025-02-17 22:02:27,151] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-02-17 22:02:27,151] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-02-17 22:02:27,151] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2025-02-17 22:02:27,151] [INFO] [config.py:1003:print]   amp_params ................... False
[2025-02-17 22:02:27,152] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-02-17 22:02:27,152] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2025-02-17 22:02:27,152] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2025-02-17 22:02:27,152] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2025-02-17 22:02:27,152] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2025-02-17 22:02:27,152] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2025-02-17 22:02:27,152] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe5184394b0>
[2025-02-17 22:02:27,152] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2025-02-17 22:02:27,152] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-02-17 22:02:27,152] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2025-02-17 22:02:27,152] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2025-02-17 22:02:27,152] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-02-17 22:02:27,152] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2025-02-17 22:02:27,152] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2025-02-17 22:02:27,152] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2025-02-17 22:02:27,152] [INFO] [config.py:1003:print]   dump_state ................... False
[2025-02-17 22:02:27,152] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   global_rank .................. 0
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 32
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-02-17 22:02:27,153] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   pld_params ................... False
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2025-02-17 22:02:27,154] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2025-02-17 22:02:27,155] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2025-02-17 22:02:27,155] [INFO] [config.py:1003:print]   train_batch_size ............. 128
[2025-02-17 22:02:27,155] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  2
[2025-02-17 22:02:27,155] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2025-02-17 22:02:27,155] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2025-02-17 22:02:27,155] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2025-02-17 22:02:27,155] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2025-02-17 22:02:27,155] [INFO] [config.py:1003:print]   world_size ................... 2
[2025-02-17 22:02:27,155] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True
[2025-02-17 22:02:27,155] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-02-17 22:02:27,155] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2025-02-17 22:02:27,155] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2025-02-17 22:02:27,155] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[2025-02-17 22:02:27,155] [INFO] [config.py:989:print_user_config]   json = {
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 1.677722e+07, 
        "stage3_prefetch_bucket_size": 1.509949e+07, 
        "stage3_param_persistence_threshold": 4.096000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true, 
        "offload_param": {
            "device": "cpu"
        }, 
        "offload_optimizer": {
            "device": "cpu"
        }
    }, 
    "gradient_accumulation_steps": 32, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 2, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
02/17/2025 22:02:27 - INFO - __main__ - ***** Running training *****
02/17/2025 22:02:27 - INFO - __main__ -   Num examples = 28578
02/17/2025 22:02:27 - INFO - __main__ -   Num Epochs = 2
02/17/2025 22:02:27 - INFO - __main__ -   Instantaneous batch size per device = 2
02/17/2025 22:02:27 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 128
02/17/2025 22:02:27 - INFO - __main__ -   Gradient Accumulation steps = 32
02/17/2025 22:02:27 - INFO - __main__ -   Total optimization steps = 448
  0%|          | 0/448 [00:00<?, ?it/s]/home/wangyatong/anaconda3/envs/proxy/lib/python3.10/site-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)
  batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)
/home/wangyatong/anaconda3/envs/proxy/lib/python3.10/site-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)
  batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)
/home/wangyatong/anaconda3/envs/proxy/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/wangyatong/anaconda3/envs/proxy/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  0%|          | 1/448 [02:39<19:45:21, 159.11s/it]02/17/2025 22:05:06 - INFO - __main__ -   Step: 1, LR: 7.692307692307694e-06, Loss: 1.6008750200271606
  0%|          | 2/448 [05:10<19:09:54, 154.70s/it]02/17/2025 22:07:37 - INFO - __main__ -   Step: 2, LR: 1.5384615384615387e-05, Loss: 1.5514885187149048
  1%|          | 3/448 [07:39<18:46:11, 151.84s/it]02/17/2025 22:10:06 - INFO - __main__ -   Step: 3, LR: 2.307692307692308e-05, Loss: 1.5629823207855225
  1%|          | 4/448 [10:08<18:34:59, 150.67s/it]02/17/2025 22:12:35 - INFO - __main__ -   Step: 4, LR: 3.0769230769230774e-05, Loss: 1.6090569496154785
  1%|          | 5/448 [12:41<18:41:07, 151.85s/it]02/17/2025 22:15:09 - INFO - __main__ -   Step: 5, LR: 3.846153846153846e-05, Loss: 1.5941274166107178
  1%|▏         | 6/448 [15:11<18:33:40, 151.18s/it]02/17/2025 22:17:39 - INFO - __main__ -   Step: 6, LR: 4.615384615384616e-05, Loss: 1.616553544998169
  2%|▏         | 7/448 [17:37<18:18:21, 149.44s/it]02/17/2025 22:20:04 - INFO - __main__ -   Step: 7, LR: 5.384615384615385e-05, Loss: 1.6331897974014282
  2%|▏         | 8/448 [20:13<18:30:31, 151.44s/it]02/17/2025 22:22:40 - INFO - __main__ -   Step: 8, LR: 6.153846153846155e-05, Loss: 1.6748332977294922
  2%|▏         | 9/448 [22:41<18:19:48, 150.31s/it]02/17/2025 22:25:08 - INFO - __main__ -   Step: 9, LR: 6.923076923076924e-05, Loss: 1.6077196598052979
  2%|▏         | 10/448 [25:07<18:08:07, 149.06s/it]02/17/2025 22:27:34 - INFO - __main__ -   Step: 10, LR: 7.692307692307693e-05, Loss: 1.598941683769226
  2%|▏         | 11/448 [27:28<17:46:54, 146.49s/it]02/17/2025 22:29:55 - INFO - __main__ -   Step: 11, LR: 8.461538461538461e-05, Loss: 1.5810761451721191
  3%|▎         | 12/448 [29:54<17:43:12, 146.31s/it]02/17/2025 22:32:21 - INFO - __main__ -   Step: 12, LR: 9.230769230769232e-05, Loss: 1.5912189483642578
  3%|▎         | 13/448 [32:25<17:50:54, 147.71s/it]02/17/2025 22:34:52 - INFO - __main__ -   Step: 13, LR: 0.0001, Loss: 1.6571383476257324
  3%|▎         | 14/448 [34:48<17:39:43, 146.51s/it]02/17/2025 22:37:15 - INFO - __main__ -   Step: 14, LR: 9.976958525345623e-05, Loss: 1.6010174751281738
  3%|▎         | 15/448 [37:16<17:39:02, 146.75s/it]02/17/2025 22:39:43 - INFO - __main__ -   Step: 15, LR: 9.953917050691245e-05, Loss: 1.615271806716919
  4%|▎         | 16/448 [39:40<17:32:02, 146.12s/it]02/17/2025 22:42:07 - INFO - __main__ -   Step: 16, LR: 9.930875576036867e-05, Loss: 1.6397570371627808
  4%|▍         | 17/448 [42:12<17:42:27, 147.91s/it]02/17/2025 22:44:39 - INFO - __main__ -   Step: 17, LR: 9.907834101382489e-05, Loss: 1.6349231004714966
  4%|▍         | 18/448 [44:36<17:31:59, 146.79s/it]02/17/2025 22:47:04 - INFO - __main__ -   Step: 18, LR: 9.884792626728111e-05, Loss: 1.615232229232788
  4%|▍         | 19/448 [47:08<17:40:15, 148.29s/it]02/17/2025 22:49:35 - INFO - __main__ -   Step: 19, LR: 9.861751152073733e-05, Loss: 1.587170124053955
  4%|▍         | 20/448 [49:35<17:33:58, 147.75s/it]02/17/2025 22:52:02 - INFO - __main__ -   Step: 20, LR: 9.838709677419355e-05, Loss: 1.5247424840927124
  5%|▍         | 21/448 [52:04<17:33:41, 148.06s/it]02/17/2025 22:54:31 - INFO - __main__ -   Step: 21, LR: 9.815668202764977e-05, Loss: 1.5472602844238281
  5%|▍         | 22/448 [54:27<17:21:52, 146.74s/it]02/17/2025 22:56:54 - INFO - __main__ -   Step: 22, LR: 9.7926267281106e-05, Loss: 1.5996668338775635
  5%|▌         | 23/448 [56:54<17:20:24, 146.88s/it]02/17/2025 22:59:22 - INFO - __main__ -   Step: 23, LR: 9.769585253456221e-05, Loss: 1.5720789432525635
  5%|▌         | 24/448 [59:23<17:21:38, 147.40s/it]02/17/2025 23:01:50 - INFO - __main__ -   Step: 24, LR: 9.746543778801845e-05, Loss: 1.6277189254760742
  6%|▌         | 25/448 [1:01:54<17:27:26, 148.57s/it]02/17/2025 23:04:21 - INFO - __main__ -   Step: 25, LR: 9.723502304147467e-05, Loss: 1.5796380043029785
  6%|▌         | 26/448 [1:04:27<17:32:55, 149.71s/it]02/17/2025 23:06:54 - INFO - __main__ -   Step: 26, LR: 9.700460829493087e-05, Loss: 1.5333832502365112
  6%|▌         | 27/448 [1:06:52<17:20:19, 148.27s/it]02/17/2025 23:09:19 - INFO - __main__ -   Step: 27, LR: 9.677419354838711e-05, Loss: 1.5374271869659424
  6%|▋         | 28/448 [1:09:21<17:20:08, 148.59s/it]02/17/2025 23:11:48 - INFO - __main__ -   Step: 28, LR: 9.654377880184331e-05, Loss: 1.5569087266921997
  6%|▋         | 29/448 [1:11:45<17:08:25, 147.27s/it]02/17/2025 23:14:12 - INFO - __main__ -   Step: 29, LR: 9.631336405529955e-05, Loss: 1.5994350910186768
  7%|▋         | 30/448 [1:14:09<16:57:55, 146.11s/it]02/17/2025 23:16:36 - INFO - __main__ -   Step: 30, LR: 9.608294930875577e-05, Loss: 1.5995395183563232
  7%|▋         | 31/448 [1:16:37<16:59:57, 146.76s/it]02/17/2025 23:19:04 - INFO - __main__ -   Step: 31, LR: 9.585253456221198e-05, Loss: 1.5971719026565552
  7%|▋         | 32/448 [1:19:00<16:50:40, 145.77s/it]02/17/2025 23:21:27 - INFO - __main__ -   Step: 32, LR: 9.562211981566821e-05, Loss: 1.5860323905944824
  7%|▋         | 33/448 [1:21:29<16:55:21, 146.80s/it]02/17/2025 23:23:57 - INFO - __main__ -   Step: 33, LR: 9.539170506912443e-05, Loss: 1.5358315706253052
  8%|▊         | 34/448 [1:23:52<16:44:31, 145.58s/it]02/17/2025 23:26:19 - INFO - __main__ -   Step: 34, LR: 9.516129032258065e-05, Loss: 1.5977905988693237
  8%|▊         | 35/448 [1:26:21<16:48:16, 146.48s/it]02/17/2025 23:28:48 - INFO - __main__ -   Step: 35, LR: 9.493087557603687e-05, Loss: 1.5414788722991943
  8%|▊         | 36/448 [1:28:45<16:41:14, 145.81s/it]02/17/2025 23:31:12 - INFO - __main__ -   Step: 36, LR: 9.47004608294931e-05, Loss: 1.5142836570739746
  8%|▊         | 37/448 [1:31:06<16:28:10, 144.26s/it]02/17/2025 23:33:33 - INFO - __main__ -   Step: 37, LR: 9.447004608294931e-05, Loss: 1.5806828737258911
  8%|▊         | 38/448 [1:33:31<16:28:11, 144.61s/it]02/17/2025 23:35:58 - INFO - __main__ -   Step: 38, LR: 9.423963133640553e-05, Loss: 1.5702321529388428
  9%|▊         | 39/448 [1:36:00<16:35:34, 146.05s/it]02/17/2025 23:38:28 - INFO - __main__ -   Step: 39, LR: 9.400921658986176e-05, Loss: 1.6023821830749512
  9%|▉         | 40/448 [1:38:28<16:36:21, 146.52s/it]02/17/2025 23:40:55 - INFO - __main__ -   Step: 40, LR: 9.377880184331798e-05, Loss: 1.5992693901062012
  9%|▉         | 41/448 [1:40:53<16:30:08, 145.97s/it]02/17/2025 23:43:20 - INFO - __main__ -   Step: 41, LR: 9.35483870967742e-05, Loss: 1.577460765838623
  9%|▉         | 42/448 [1:43:15<16:19:52, 144.81s/it]02/17/2025 23:45:42 - INFO - __main__ -   Step: 42, LR: 9.331797235023042e-05, Loss: 1.5601791143417358
 10%|▉         | 43/448 [1:45:40<16:17:37, 144.83s/it]02/17/2025 23:48:07 - INFO - __main__ -   Step: 43, LR: 9.308755760368664e-05, Loss: 1.5659630298614502
 10%|▉         | 44/448 [1:48:07<16:20:48, 145.66s/it]02/17/2025 23:50:35 - INFO - __main__ -   Step: 44, LR: 9.285714285714286e-05, Loss: 1.603583574295044
 10%|█         | 45/448 [1:50:31<16:14:48, 145.13s/it]02/17/2025 23:52:58 - INFO - __main__ -   Step: 45, LR: 9.262672811059908e-05, Loss: 1.5582594871520996
 10%|█         | 46/448 [1:52:56<16:12:18, 145.12s/it]02/17/2025 23:55:24 - INFO - __main__ -   Step: 46, LR: 9.239631336405531e-05, Loss: 1.5977108478546143
 10%|█         | 47/448 [1:55:20<16:07:28, 144.76s/it]02/17/2025 23:57:47 - INFO - __main__ -   Step: 47, LR: 9.216589861751152e-05, Loss: 1.5560028553009033
 11%|█         | 48/448 [1:57:47<16:08:19, 145.25s/it]02/18/2025 00:00:14 - INFO - __main__ -   Step: 48, LR: 9.193548387096774e-05, Loss: 1.5573368072509766
 11%|█         | 49/448 [2:00:13<16:07:12, 145.45s/it]02/18/2025 00:02:40 - INFO - __main__ -   Step: 49, LR: 9.170506912442398e-05, Loss: 1.5601078271865845
 11%|█         | 50/448 [2:02:36<16:00:58, 144.87s/it]02/18/2025 00:05:03 - INFO - __main__ -   Step: 50, LR: 9.147465437788018e-05, Loss: 1.6021368503570557
 11%|█▏        | 51/448 [2:05:00<15:57:15, 144.67s/it]02/18/2025 00:07:27 - INFO - __main__ -   Step: 51, LR: 9.124423963133642e-05, Loss: 1.5774222612380981
 12%|█▏        | 52/448 [2:07:26<15:56:42, 144.96s/it]02/18/2025 00:09:53 - INFO - __main__ -   Step: 52, LR: 9.101382488479264e-05, Loss: 1.55008864402771
 12%|█▏        | 53/448 [2:09:52<15:56:09, 145.24s/it]02/18/2025 00:12:19 - INFO - __main__ -   Step: 53, LR: 9.078341013824884e-05, Loss: 1.590093731880188
 12%|█▏        | 54/448 [2:12:14<15:47:52, 144.35s/it]02/18/2025 00:14:41 - INFO - __main__ -   Step: 54, LR: 9.055299539170508e-05, Loss: 1.5793488025665283
 12%|█▏        | 55/448 [2:14:36<15:41:36, 143.76s/it]02/18/2025 00:17:04 - INFO - __main__ -   Step: 55, LR: 9.032258064516129e-05, Loss: 1.5747876167297363
 12%|█▎        | 56/448 [2:16:55<15:29:51, 142.33s/it]02/18/2025 00:19:23 - INFO - __main__ -   Step: 56, LR: 9.009216589861752e-05, Loss: 1.5349445343017578
 13%|█▎        | 57/448 [2:19:20<15:32:23, 143.08s/it]02/18/2025 00:21:47 - INFO - __main__ -   Step: 57, LR: 8.986175115207374e-05, Loss: 1.5917809009552002
 13%|█▎        | 58/448 [2:21:45<15:32:52, 143.52s/it]02/18/2025 00:24:12 - INFO - __main__ -   Step: 58, LR: 8.963133640552995e-05, Loss: 1.5942474603652954
 13%|█▎        | 59/448 [2:24:09<15:32:16, 143.80s/it]02/18/2025 00:26:36 - INFO - __main__ -   Step: 59, LR: 8.940092165898618e-05, Loss: 1.5664951801300049
 13%|█▎        | 60/448 [2:26:34<15:31:11, 144.00s/it]02/18/2025 00:29:01 - INFO - __main__ -   Step: 60, LR: 8.91705069124424e-05, Loss: 1.6078407764434814
 14%|█▎        | 61/448 [2:28:56<15:25:56, 143.56s/it]02/18/2025 00:31:23 - INFO - __main__ -   Step: 61, LR: 8.894009216589862e-05, Loss: 1.6052300930023193
 14%|█▍        | 62/448 [2:31:18<15:20:21, 143.06s/it]02/18/2025 00:33:45 - INFO - __main__ -   Step: 62, LR: 8.870967741935484e-05, Loss: 1.5795142650604248
 14%|█▍        | 63/448 [2:33:47<15:29:31, 144.86s/it]02/18/2025 00:36:14 - INFO - __main__ -   Step: 63, LR: 8.847926267281106e-05, Loss: 1.573633074760437
 14%|█▍        | 64/448 [2:36:16<15:35:02, 146.10s/it]02/18/2025 00:38:43 - INFO - __main__ -   Step: 64, LR: 8.824884792626729e-05, Loss: 1.563907265663147
 15%|█▍        | 65/448 [2:38:42<15:30:59, 145.85s/it]02/18/2025 00:41:09 - INFO - __main__ -   Step: 65, LR: 8.80184331797235e-05, Loss: 1.5910619497299194
 15%|█▍        | 66/448 [2:41:07<15:28:35, 145.85s/it]02/18/2025 00:43:35 - INFO - __main__ -   Step: 66, LR: 8.778801843317973e-05, Loss: 1.5843098163604736
 15%|█▍        | 67/448 [2:43:28<15:16:02, 144.26s/it]02/18/2025 00:45:55 - INFO - __main__ -   Step: 67, LR: 8.755760368663595e-05, Loss: 1.5742590427398682
 15%|█▌        | 68/448 [2:45:51<15:11:38, 143.94s/it]02/18/2025 00:48:18 - INFO - __main__ -   Step: 68, LR: 8.732718894009218e-05, Loss: 1.5983455181121826
 15%|█▌        | 69/448 [2:48:16<15:11:33, 144.31s/it]02/18/2025 00:50:43 - INFO - __main__ -   Step: 69, LR: 8.709677419354839e-05, Loss: 1.5567030906677246
 16%|█▌        | 70/448 [2:50:38<15:04:17, 143.54s/it]02/18/2025 00:53:05 - INFO - __main__ -   Step: 70, LR: 8.686635944700461e-05, Loss: 1.5378810167312622
 16%|█▌        | 71/448 [2:53:02<15:02:21, 143.61s/it]02/18/2025 00:55:29 - INFO - __main__ -   Step: 71, LR: 8.663594470046083e-05, Loss: 1.5279771089553833
 16%|█▌        | 72/448 [2:55:26<15:01:04, 143.79s/it]02/18/2025 00:57:53 - INFO - __main__ -   Step: 72, LR: 8.640552995391705e-05, Loss: 1.6161317825317383
 16%|█▋        | 73/448 [2:57:51<15:00:02, 144.01s/it]02/18/2025 01:00:18 - INFO - __main__ -   Step: 73, LR: 8.617511520737328e-05, Loss: 1.5749250650405884
 17%|█▋        | 74/448 [3:00:16<15:00:41, 144.50s/it]02/18/2025 01:02:43 - INFO - __main__ -   Step: 74, LR: 8.594470046082949e-05, Loss: 1.6065932512283325
 17%|█▋        | 75/448 [3:02:36<14:50:02, 143.17s/it]02/18/2025 01:05:03 - INFO - __main__ -   Step: 75, LR: 8.571428571428571e-05, Loss: 1.5373611450195312
 17%|█▋        | 76/448 [3:05:05<14:58:55, 144.99s/it]02/18/2025 01:07:33 - INFO - __main__ -   Step: 76, LR: 8.548387096774195e-05, Loss: 1.5971179008483887
 17%|█▋        | 77/448 [3:07:31<14:58:23, 145.29s/it]02/18/2025 01:09:59 - INFO - __main__ -   Step: 77, LR: 8.525345622119815e-05, Loss: 1.5618126392364502
 17%|█▋        | 78/448 [3:09:54<14:51:28, 144.56s/it]02/18/2025 01:12:22 - INFO - __main__ -   Step: 78, LR: 8.502304147465439e-05, Loss: 1.5664781332015991
 18%|█▊        | 79/448 [3:12:18<14:47:42, 144.34s/it]02/18/2025 01:14:45 - INFO - __main__ -   Step: 79, LR: 8.479262672811061e-05, Loss: 1.5418004989624023
 18%|█▊        | 80/448 [3:14:43<14:46:46, 144.58s/it]02/18/2025 01:17:10 - INFO - __main__ -   Step: 80, LR: 8.456221198156682e-05, Loss: 1.5590198040008545
 18%|█▊        | 81/448 [3:17:06<14:41:29, 144.11s/it]02/18/2025 01:19:33 - INFO - __main__ -   Step: 81, LR: 8.433179723502305e-05, Loss: 1.5449484586715698
 18%|█▊        | 82/448 [3:19:26<14:31:29, 142.87s/it]02/18/2025 01:21:53 - INFO - __main__ -   Step: 82, LR: 8.410138248847926e-05, Loss: 1.5146021842956543
 19%|█▊        | 83/448 [3:21:46<14:24:08, 142.05s/it]02/18/2025 01:24:14 - INFO - __main__ -   Step: 83, LR: 8.387096774193549e-05, Loss: 1.5201940536499023
 19%|█▉        | 84/448 [3:24:09<14:23:02, 142.26s/it]02/18/2025 01:26:36 - INFO - __main__ -   Step: 84, LR: 8.364055299539171e-05, Loss: 1.6442742347717285
 19%|█▉        | 85/448 [3:26:32<14:22:17, 142.53s/it]02/18/2025 01:28:59 - INFO - __main__ -   Step: 85, LR: 8.341013824884793e-05, Loss: 1.622281789779663
 19%|█▉        | 86/448 [3:29:01<14:30:13, 144.24s/it]02/18/2025 01:31:28 - INFO - __main__ -   Step: 86, LR: 8.317972350230415e-05, Loss: 1.5923069715499878
 19%|█▉        | 87/448 [3:31:28<14:33:00, 145.10s/it]02/18/2025 01:33:55 - INFO - __main__ -   Step: 87, LR: 8.294930875576037e-05, Loss: 1.487220287322998
 20%|█▉        | 88/448 [3:33:51<14:27:22, 144.56s/it]02/18/2025 01:36:18 - INFO - __main__ -   Step: 88, LR: 8.27188940092166e-05, Loss: 1.5755364894866943
 20%|█▉        | 89/448 [3:36:12<14:18:13, 143.43s/it]02/18/2025 01:38:39 - INFO - __main__ -   Step: 89, LR: 8.248847926267282e-05, Loss: 1.5433814525604248
 20%|██        | 90/448 [3:38:32<14:10:22, 142.52s/it]02/18/2025 01:40:59 - INFO - __main__ -   Step: 90, LR: 8.225806451612904e-05, Loss: 1.5255024433135986
 20%|██        | 91/448 [3:40:54<14:07:01, 142.36s/it]02/18/2025 01:43:21 - INFO - __main__ -   Step: 91, LR: 8.202764976958526e-05, Loss: 1.5247666835784912
 21%|██        | 92/448 [3:43:16<14:03:42, 142.20s/it]02/18/2025 01:45:43 - INFO - __main__ -   Step: 92, LR: 8.179723502304148e-05, Loss: 1.5381466150283813
 21%|██        | 93/448 [3:45:39<14:02:20, 142.37s/it]02/18/2025 01:48:06 - INFO - __main__ -   Step: 93, LR: 8.15668202764977e-05, Loss: 1.5597827434539795
 21%|██        | 94/448 [3:48:04<14:04:26, 143.12s/it]02/18/2025 01:50:31 - INFO - __main__ -   Step: 94, LR: 8.133640552995392e-05, Loss: 1.5689420700073242
 21%|██        | 95/448 [3:50:32<14:11:51, 144.79s/it]02/18/2025 01:52:59 - INFO - __main__ -   Step: 95, LR: 8.110599078341015e-05, Loss: 1.5897901058197021
 21%|██▏       | 96/448 [3:52:54<14:04:14, 143.90s/it]02/18/2025 01:55:21 - INFO - __main__ -   Step: 96, LR: 8.087557603686636e-05, Loss: 1.6198439598083496
 22%|██▏       | 97/448 [3:55:23<14:10:34, 145.40s/it]02/18/2025 01:57:50 - INFO - __main__ -   Step: 97, LR: 8.064516129032258e-05, Loss: 1.5577714443206787
 22%|██▏       | 98/448 [3:57:44<13:59:55, 143.99s/it]02/18/2025 02:00:11 - INFO - __main__ -   Step: 98, LR: 8.04147465437788e-05, Loss: 1.520009994506836
 22%|██▏       | 99/448 [4:00:05<13:53:15, 143.25s/it]02/18/2025 02:02:32 - INFO - __main__ -   Step: 99, LR: 8.018433179723502e-05, Loss: 1.600285291671753
 22%|██▏       | 100/448 [4:02:32<13:57:39, 144.42s/it]02/18/2025 02:05:00 - INFO - __main__ -   Step: 100, LR: 7.995391705069126e-05, Loss: 1.5528615713119507
 23%|██▎       | 101/448 [4:04:52<13:47:25, 143.07s/it]02/18/2025 02:07:19 - INFO - __main__ -   Step: 101, LR: 7.972350230414746e-05, Loss: 1.5157504081726074
 23%|██▎       | 102/448 [4:07:15<13:44:58, 143.06s/it]02/18/2025 02:09:43 - INFO - __main__ -   Step: 102, LR: 7.94930875576037e-05, Loss: 1.5651729106903076
 23%|██▎       | 103/448 [4:09:36<13:38:06, 142.28s/it]02/18/2025 02:12:03 - INFO - __main__ -   Step: 103, LR: 7.926267281105992e-05, Loss: 1.5234854221343994
 23%|██▎       | 104/448 [4:11:55<13:30:59, 141.45s/it]02/18/2025 02:14:23 - INFO - __main__ -   Step: 104, LR: 7.903225806451613e-05, Loss: 1.5284076929092407
 23%|██▎       | 105/448 [4:14:13<13:22:37, 140.40s/it]02/18/2025 02:16:40 - INFO - __main__ -   Step: 105, LR: 7.880184331797236e-05, Loss: 1.5713703632354736
 24%|██▎       | 106/448 [4:16:41<13:33:15, 142.68s/it]02/18/2025 02:19:08 - INFO - __main__ -   Step: 106, LR: 7.857142857142858e-05, Loss: 1.5402418375015259
 24%|██▍       | 107/448 [4:19:02<13:27:52, 142.15s/it]02/18/2025 02:21:29 - INFO - __main__ -   Step: 107, LR: 7.83410138248848e-05, Loss: 1.5553960800170898
 24%|██▍       | 108/448 [4:21:26<13:28:39, 142.70s/it]02/18/2025 02:23:53 - INFO - __main__ -   Step: 108, LR: 7.811059907834102e-05, Loss: 1.5163013935089111
 24%|██▍       | 109/448 [4:23:47<13:22:53, 142.11s/it]02/18/2025 02:26:14 - INFO - __main__ -   Step: 109, LR: 7.788018433179723e-05, Loss: 1.5001686811447144
 25%|██▍       | 110/448 [4:26:11<13:23:32, 142.64s/it]02/18/2025 02:28:38 - INFO - __main__ -   Step: 110, LR: 7.764976958525346e-05, Loss: 1.5925872325897217
 25%|██▍       | 111/448 [4:28:36<13:25:34, 143.43s/it]02/18/2025 02:31:03 - INFO - __main__ -   Step: 111, LR: 7.741935483870968e-05, Loss: 1.5821936130523682
 25%|██▌       | 112/448 [4:30:58<13:21:04, 143.05s/it]02/18/2025 02:33:25 - INFO - __main__ -   Step: 112, LR: 7.71889400921659e-05, Loss: 1.5100336074829102
 25%|██▌       | 113/448 [4:33:18<13:13:13, 142.07s/it]02/18/2025 02:35:45 - INFO - __main__ -   Step: 113, LR: 7.695852534562212e-05, Loss: 1.6154732704162598
 25%|██▌       | 114/448 [4:35:39<13:08:54, 141.72s/it]02/18/2025 02:38:06 - INFO - __main__ -   Step: 114, LR: 7.672811059907835e-05, Loss: 1.5417399406433105
 26%|██▌       | 115/448 [4:37:59<13:04:18, 141.32s/it]02/18/2025 02:40:26 - INFO - __main__ -   Step: 115, LR: 7.649769585253457e-05, Loss: 1.5382089614868164
 26%|██▌       | 116/448 [4:40:24<13:08:09, 142.44s/it]02/18/2025 02:42:52 - INFO - __main__ -   Step: 116, LR: 7.626728110599079e-05, Loss: 1.5584737062454224
 26%|██▌       | 117/448 [4:42:49<13:10:09, 143.23s/it]02/18/2025 02:45:17 - INFO - __main__ -   Step: 117, LR: 7.603686635944701e-05, Loss: 1.5413024425506592
 26%|██▋       | 118/448 [4:45:13<13:08:33, 143.38s/it]02/18/2025 02:47:40 - INFO - __main__ -   Step: 118, LR: 7.580645161290323e-05, Loss: 1.5743229389190674
 27%|██▋       | 119/448 [4:47:35<13:03:17, 142.85s/it]02/18/2025 02:50:02 - INFO - __main__ -   Step: 119, LR: 7.557603686635945e-05, Loss: 1.5279817581176758
 27%|██▋       | 120/448 [4:49:53<12:53:52, 141.56s/it]02/18/2025 02:52:20 - INFO - __main__ -   Step: 120, LR: 7.534562211981567e-05, Loss: 1.5951635837554932
 27%|██▋       | 121/448 [4:52:12<12:47:28, 140.82s/it]02/18/2025 02:54:40 - INFO - __main__ -   Step: 121, LR: 7.511520737327189e-05, Loss: 1.5716423988342285
 27%|██▋       | 122/448 [4:54:33<12:44:15, 140.66s/it]02/18/2025 02:57:00 - INFO - __main__ -   Step: 122, LR: 7.488479262672812e-05, Loss: 1.5945587158203125
 27%|██▋       | 123/448 [4:56:56<12:46:02, 141.42s/it]02/18/2025 02:59:23 - INFO - __main__ -   Step: 123, LR: 7.465437788018433e-05, Loss: 1.595027208328247
 28%|██▊       | 124/448 [4:59:18<12:44:21, 141.55s/it]02/18/2025 03:01:45 - INFO - __main__ -   Step: 124, LR: 7.442396313364057e-05, Loss: 1.5264583826065063
 28%|██▊       | 125/448 [5:01:42<12:47:00, 142.48s/it]02/18/2025 03:04:10 - INFO - __main__ -   Step: 125, LR: 7.419354838709677e-05, Loss: 1.4927897453308105
 28%|██▊       | 126/448 [5:04:04<12:43:46, 142.32s/it]02/18/2025 03:06:31 - INFO - __main__ -   Step: 126, LR: 7.396313364055299e-05, Loss: 1.6002694368362427
 28%|██▊       | 127/448 [5:06:23<12:34:49, 141.09s/it]02/18/2025 03:08:50 - INFO - __main__ -   Step: 127, LR: 7.373271889400923e-05, Loss: 1.5278868675231934
 29%|██▊       | 128/448 [5:08:43<12:31:29, 140.91s/it]02/18/2025 03:11:10 - INFO - __main__ -   Step: 128, LR: 7.350230414746543e-05, Loss: 1.4994480609893799
 29%|██▉       | 129/448 [5:11:04<12:29:23, 140.95s/it]02/18/2025 03:13:31 - INFO - __main__ -   Step: 129, LR: 7.327188940092167e-05, Loss: 1.5914452075958252
 29%|██▉       | 130/448 [5:13:25<12:26:12, 140.79s/it]02/18/2025 03:15:52 - INFO - __main__ -   Step: 130, LR: 7.304147465437789e-05, Loss: 1.5313310623168945
 29%|██▉       | 131/448 [5:15:46<12:25:13, 141.05s/it]02/18/2025 03:18:13 - INFO - __main__ -   Step: 131, LR: 7.28110599078341e-05, Loss: 1.5430155992507935
 29%|██▉       | 132/448 [5:18:06<12:20:57, 140.69s/it]02/18/2025 03:20:33 - INFO - __main__ -   Step: 132, LR: 7.258064516129033e-05, Loss: 1.5627176761627197
 30%|██▉       | 133/448 [5:20:30<12:23:42, 141.66s/it]02/18/2025 03:22:57 - INFO - __main__ -   Step: 133, LR: 7.235023041474655e-05, Loss: 1.5706309080123901
 30%|██▉       | 134/448 [5:22:55<12:27:26, 142.82s/it]02/18/2025 03:25:23 - INFO - __main__ -   Step: 134, LR: 7.211981566820277e-05, Loss: 1.5000776052474976
 30%|███       | 135/448 [5:25:21<12:28:58, 143.57s/it]02/18/2025 03:27:48 - INFO - __main__ -   Step: 135, LR: 7.188940092165899e-05, Loss: 1.5401115417480469
 30%|███       | 136/448 [5:27:47<12:30:26, 144.32s/it]02/18/2025 03:30:14 - INFO - __main__ -   Step: 136, LR: 7.16589861751152e-05, Loss: 1.566216230392456
 31%|███       | 137/448 [5:30:06<12:19:35, 142.69s/it]02/18/2025 03:32:33 - INFO - __main__ -   Step: 137, LR: 7.142857142857143e-05, Loss: 1.554180383682251
 31%|███       | 138/448 [5:32:30<12:19:35, 143.15s/it]02/18/2025 03:34:57 - INFO - __main__ -   Step: 138, LR: 7.119815668202765e-05, Loss: 1.5438233613967896
 31%|███       | 139/448 [5:34:55<12:19:24, 143.57s/it]02/18/2025 03:37:22 - INFO - __main__ -   Step: 139, LR: 7.096774193548388e-05, Loss: 1.5338544845581055
 31%|███▏      | 140/448 [5:37:16<12:14:28, 143.08s/it]02/18/2025 03:39:44 - INFO - __main__ -   Step: 140, LR: 7.07373271889401e-05, Loss: 1.6270793676376343
 31%|███▏      | 141/448 [5:39:38<12:10:29, 142.77s/it]02/18/2025 03:42:06 - INFO - __main__ -   Step: 141, LR: 7.050691244239632e-05, Loss: 1.5416733026504517
 32%|███▏      | 142/448 [5:41:59<12:04:15, 142.01s/it]02/18/2025 03:44:26 - INFO - __main__ -   Step: 142, LR: 7.027649769585254e-05, Loss: 1.5659546852111816
 32%|███▏      | 143/448 [5:44:24<12:06:13, 142.86s/it]02/18/2025 03:46:51 - INFO - __main__ -   Step: 143, LR: 7.004608294930876e-05, Loss: 1.5534021854400635
 32%|███▏      | 144/448 [5:46:46<12:03:35, 142.81s/it]02/18/2025 03:49:13 - INFO - __main__ -   Step: 144, LR: 6.981566820276498e-05, Loss: 1.5057488679885864
 32%|███▏      | 145/448 [5:49:10<12:02:55, 143.15s/it]02/18/2025 03:51:37 - INFO - __main__ -   Step: 145, LR: 6.95852534562212e-05, Loss: 1.557963252067566
 33%|███▎      | 146/448 [5:51:31<11:56:18, 142.31s/it]02/18/2025 03:53:58 - INFO - __main__ -   Step: 146, LR: 6.935483870967743e-05, Loss: 1.5474390983581543
 33%|███▎      | 147/448 [5:53:57<12:00:41, 143.66s/it]02/18/2025 03:56:25 - INFO - __main__ -   Step: 147, LR: 6.912442396313364e-05, Loss: 1.5520381927490234
 33%|███▎      | 148/448 [5:56:24<12:02:23, 144.48s/it]02/18/2025 03:58:51 - INFO - __main__ -   Step: 148, LR: 6.889400921658986e-05, Loss: 1.5670814514160156
 33%|███▎      | 149/448 [5:58:46<11:56:25, 143.76s/it]02/18/2025 04:01:13 - INFO - __main__ -   Step: 149, LR: 6.86635944700461e-05, Loss: 1.5874097347259521
 33%|███▎      | 150/448 [6:01:04<11:46:01, 142.15s/it]02/18/2025 04:03:31 - INFO - __main__ -   Step: 150, LR: 6.84331797235023e-05, Loss: 1.5711817741394043
 34%|███▎      | 151/448 [6:03:28<11:46:00, 142.63s/it]02/18/2025 04:05:55 - INFO - __main__ -   Step: 151, LR: 6.820276497695854e-05, Loss: 1.5709329843521118
 34%|███▍      | 152/448 [6:05:51<11:43:29, 142.60s/it]02/18/2025 04:08:18 - INFO - __main__ -   Step: 152, LR: 6.797235023041474e-05, Loss: 1.5439622402191162
 34%|███▍      | 153/448 [6:08:11<11:37:48, 141.93s/it]02/18/2025 04:10:38 - INFO - __main__ -   Step: 153, LR: 6.774193548387096e-05, Loss: 1.5637378692626953
 34%|███▍      | 154/448 [6:10:33<11:36:14, 142.09s/it]02/18/2025 04:13:01 - INFO - __main__ -   Step: 154, LR: 6.75115207373272e-05, Loss: 1.5520284175872803
 35%|███▍      | 155/448 [6:12:51<11:27:32, 140.80s/it]02/18/2025 04:15:18 - INFO - __main__ -   Step: 155, LR: 6.72811059907834e-05, Loss: 1.5489518642425537
 35%|███▍      | 156/448 [6:15:09<11:21:38, 140.06s/it]02/18/2025 04:17:37 - INFO - __main__ -   Step: 156, LR: 6.705069124423964e-05, Loss: 1.5289117097854614
 35%|███▌      | 157/448 [6:17:34<11:25:05, 141.26s/it]02/18/2025 04:20:01 - INFO - __main__ -   Step: 157, LR: 6.682027649769586e-05, Loss: 1.5484375953674316
 35%|███▌      | 158/448 [6:19:55<11:22:46, 141.26s/it]02/18/2025 04:22:22 - INFO - __main__ -   Step: 158, LR: 6.658986175115207e-05, Loss: 1.5736284255981445
 35%|███▌      | 159/448 [6:22:14<11:17:55, 140.74s/it]02/18/2025 04:24:41 - INFO - __main__ -   Step: 159, LR: 6.63594470046083e-05, Loss: 1.5501585006713867
 36%|███▌      | 160/448 [6:24:37<11:17:36, 141.17s/it]02/18/2025 04:27:04 - INFO - __main__ -   Step: 160, LR: 6.612903225806452e-05, Loss: 1.5953750610351562
 36%|███▌      | 161/448 [6:26:59<11:17:01, 141.54s/it]02/18/2025 04:29:26 - INFO - __main__ -   Step: 161, LR: 6.589861751152074e-05, Loss: 1.5747361183166504
 36%|███▌      | 162/448 [6:29:20<11:13:50, 141.37s/it]02/18/2025 04:31:47 - INFO - __main__ -   Step: 162, LR: 6.566820276497696e-05, Loss: 1.5755341053009033
 36%|███▋      | 163/448 [6:31:42<11:12:56, 141.67s/it]02/18/2025 04:34:09 - INFO - __main__ -   Step: 163, LR: 6.543778801843318e-05, Loss: 1.5210578441619873
 37%|███▋      | 164/448 [6:34:09<11:17:49, 143.20s/it]02/18/2025 04:36:36 - INFO - __main__ -   Step: 164, LR: 6.52073732718894e-05, Loss: 1.5713002681732178
 37%|███▋      | 165/448 [6:36:31<11:13:28, 142.79s/it]02/18/2025 04:38:58 - INFO - __main__ -   Step: 165, LR: 6.497695852534563e-05, Loss: 1.5475764274597168
 37%|███▋      | 166/448 [6:38:51<11:06:51, 141.89s/it]02/18/2025 04:41:18 - INFO - __main__ -   Step: 166, LR: 6.474654377880185e-05, Loss: 1.5127735137939453
 37%|███▋      | 167/448 [6:41:13<11:05:02, 142.00s/it]02/18/2025 04:43:40 - INFO - __main__ -   Step: 167, LR: 6.451612903225807e-05, Loss: 1.5660361051559448
 38%|███▊      | 168/448 [6:43:31<10:57:07, 140.81s/it]02/18/2025 04:45:58 - INFO - __main__ -   Step: 168, LR: 6.428571428571429e-05, Loss: 1.5220375061035156
 38%|███▊      | 169/448 [6:45:52<10:55:49, 141.04s/it]02/18/2025 04:48:20 - INFO - __main__ -   Step: 169, LR: 6.405529953917051e-05, Loss: 1.4955112934112549
 38%|███▊      | 170/448 [6:48:14<10:53:51, 141.12s/it]02/18/2025 04:50:41 - INFO - __main__ -   Step: 170, LR: 6.382488479262673e-05, Loss: 1.5860902070999146
 38%|███▊      | 171/448 [6:50:36<10:53:16, 141.50s/it]02/18/2025 04:53:03 - INFO - __main__ -   Step: 171, LR: 6.359447004608295e-05, Loss: 1.515725016593933
 38%|███▊      | 172/448 [6:52:55<10:47:11, 140.70s/it]02/18/2025 04:55:22 - INFO - __main__ -   Step: 172, LR: 6.336405529953917e-05, Loss: 1.5670268535614014
 39%|███▊      | 173/448 [6:55:12<10:39:38, 139.56s/it]02/18/2025 04:57:39 - INFO - __main__ -   Step: 173, LR: 6.31336405529954e-05, Loss: 1.5273643732070923
 39%|███▉      | 174/448 [6:57:34<10:41:11, 140.41s/it]02/18/2025 05:00:01 - INFO - __main__ -   Step: 174, LR: 6.290322580645161e-05, Loss: 1.5250825881958008
 39%|███▉      | 175/448 [6:59:55<10:39:34, 140.57s/it]02/18/2025 05:02:22 - INFO - __main__ -   Step: 175, LR: 6.267281105990783e-05, Loss: 1.5326145887374878
 39%|███▉      | 176/448 [7:02:16<10:36:58, 140.51s/it]02/18/2025 05:04:43 - INFO - __main__ -   Step: 176, LR: 6.244239631336407e-05, Loss: 1.5486488342285156
 40%|███▉      | 177/448 [7:04:37<10:35:40, 140.74s/it]02/18/2025 05:07:04 - INFO - __main__ -   Step: 177, LR: 6.221198156682027e-05, Loss: 1.561761736869812
 40%|███▉      | 178/448 [7:06:59<10:35:10, 141.15s/it]02/18/2025 05:09:26 - INFO - __main__ -   Step: 178, LR: 6.198156682027651e-05, Loss: 1.487220048904419
 40%|███▉      | 179/448 [7:09:20<10:32:26, 141.07s/it]02/18/2025 05:11:47 - INFO - __main__ -   Step: 179, LR: 6.175115207373272e-05, Loss: 1.5264878273010254
 40%|████      | 180/448 [7:11:42<10:31:08, 141.30s/it]02/18/2025 05:14:09 - INFO - __main__ -   Step: 180, LR: 6.152073732718894e-05, Loss: 1.4916433095932007
 40%|████      | 181/448 [7:14:06<10:32:39, 142.17s/it]02/18/2025 05:16:33 - INFO - __main__ -   Step: 181, LR: 6.129032258064517e-05, Loss: 1.559095859527588
 41%|████      | 182/448 [7:16:26<10:28:05, 141.68s/it]02/18/2025 05:18:54 - INFO - __main__ -   Step: 182, LR: 6.105990783410138e-05, Loss: 1.5494039058685303
 41%|████      | 183/448 [7:18:48<10:25:49, 141.70s/it]02/18/2025 05:21:15 - INFO - __main__ -   Step: 183, LR: 6.082949308755761e-05, Loss: 1.5163938999176025
 41%|████      | 184/448 [7:21:08<10:20:54, 141.12s/it]02/18/2025 05:23:35 - INFO - __main__ -   Step: 184, LR: 6.0599078341013825e-05, Loss: 1.5385501384735107
 41%|████▏     | 185/448 [7:23:25<10:12:37, 139.76s/it]02/18/2025 05:25:52 - INFO - __main__ -   Step: 185, LR: 6.036866359447005e-05, Loss: 1.5780863761901855
 42%|████▏     | 186/448 [7:25:45<10:11:49, 140.11s/it]02/18/2025 05:28:13 - INFO - __main__ -   Step: 186, LR: 6.013824884792627e-05, Loss: 1.5092625617980957
 42%|████▏     | 187/448 [7:28:04<10:07:04, 139.56s/it]02/18/2025 05:30:31 - INFO - __main__ -   Step: 187, LR: 5.990783410138249e-05, Loss: 1.5329396724700928
 42%|████▏     | 188/448 [7:30:29<10:11:51, 141.20s/it]02/18/2025 05:32:56 - INFO - __main__ -   Step: 188, LR: 5.9677419354838715e-05, Loss: 1.5116491317749023
 42%|████▏     | 189/448 [7:32:47<10:05:53, 140.36s/it]02/18/2025 05:35:14 - INFO - __main__ -   Step: 189, LR: 5.944700460829493e-05, Loss: 1.568676471710205
 42%|████▏     | 190/448 [7:35:10<10:06:28, 141.04s/it]02/18/2025 05:37:37 - INFO - __main__ -   Step: 190, LR: 5.921658986175116e-05, Loss: 1.4449067115783691
 43%|████▎     | 191/448 [7:37:32<10:05:36, 141.39s/it]02/18/2025 05:39:59 - INFO - __main__ -   Step: 191, LR: 5.8986175115207376e-05, Loss: 1.5490989685058594
 43%|████▎     | 192/448 [7:39:56<10:06:33, 142.16s/it]02/18/2025 05:42:23 - INFO - __main__ -   Step: 192, LR: 5.875576036866359e-05, Loss: 1.5403797626495361
 43%|████▎     | 193/448 [7:42:17<10:02:58, 141.88s/it]02/18/2025 05:44:44 - INFO - __main__ -   Step: 193, LR: 5.852534562211982e-05, Loss: 1.5788226127624512
 43%|████▎     | 194/448 [7:44:37<9:58:07, 141.29s/it] 02/18/2025 05:47:04 - INFO - __main__ -   Step: 194, LR: 5.829493087557604e-05, Loss: 1.5444953441619873
 44%|████▎     | 195/448 [7:46:56<9:52:54, 140.61s/it]02/18/2025 05:49:23 - INFO - __main__ -   Step: 195, LR: 5.8064516129032266e-05, Loss: 1.4984782934188843
 44%|████▍     | 196/448 [7:49:18<9:51:58, 140.95s/it]02/18/2025 05:51:45 - INFO - __main__ -   Step: 196, LR: 5.783410138248848e-05, Loss: 1.518326759338379
 44%|████▍     | 197/448 [7:51:43<9:54:45, 142.17s/it]02/18/2025 05:54:10 - INFO - __main__ -   Step: 197, LR: 5.76036866359447e-05, Loss: 1.5578653812408447
 44%|████▍     | 198/448 [7:54:06<9:53:13, 142.37s/it]02/18/2025 05:56:33 - INFO - __main__ -   Step: 198, LR: 5.737327188940093e-05, Loss: 1.5293537378311157
 44%|████▍     | 199/448 [7:56:28<9:50:05, 142.19s/it]02/18/2025 05:58:55 - INFO - __main__ -   Step: 199, LR: 5.714285714285714e-05, Loss: 1.5437928438186646
 45%|████▍     | 200/448 [7:58:43<9:39:43, 140.26s/it]02/18/2025 06:01:10 - INFO - __main__ -   Step: 200, LR: 5.691244239631337e-05, Loss: 1.5781015157699585
 45%|████▍     | 201/448 [8:01:05<9:39:39, 140.81s/it]02/18/2025 06:03:32 - INFO - __main__ -   Step: 201, LR: 5.668202764976959e-05, Loss: 1.4823150634765625
 45%|████▌     | 202/448 [8:03:26<9:37:05, 140.75s/it]02/18/2025 06:05:53 - INFO - __main__ -   Step: 202, LR: 5.645161290322582e-05, Loss: 1.5654120445251465
 45%|████▌     | 203/448 [8:05:44<9:31:43, 140.02s/it]02/18/2025 06:08:11 - INFO - __main__ -   Step: 203, LR: 5.622119815668203e-05, Loss: 1.5135622024536133
 46%|████▌     | 204/448 [8:08:03<9:27:31, 139.56s/it]02/18/2025 06:10:30 - INFO - __main__ -   Step: 204, LR: 5.5990783410138245e-05, Loss: 1.596969723701477
 46%|████▌     | 205/448 [8:10:24<9:26:53, 139.97s/it]02/18/2025 06:12:51 - INFO - __main__ -   Step: 205, LR: 5.576036866359447e-05, Loss: 1.5352497100830078
 46%|████▌     | 206/448 [8:12:43<9:24:08, 139.87s/it]02/18/2025 06:15:10 - INFO - __main__ -   Step: 206, LR: 5.552995391705069e-05, Loss: 1.5855703353881836
 46%|████▌     | 207/448 [8:15:04<9:22:15, 139.98s/it]02/18/2025 06:17:31 - INFO - __main__ -   Step: 207, LR: 5.529953917050692e-05, Loss: 1.5760226249694824
 46%|████▋     | 208/448 [8:17:26<9:22:17, 140.57s/it]02/18/2025 06:19:53 - INFO - __main__ -   Step: 208, LR: 5.5069124423963134e-05, Loss: 1.55922269821167
 47%|████▋     | 209/448 [8:19:45<9:18:35, 140.23s/it]02/18/2025 06:22:12 - INFO - __main__ -   Step: 209, LR: 5.4838709677419355e-05, Loss: 1.508420467376709
 47%|████▋     | 210/448 [8:22:03<9:13:46, 139.61s/it]02/18/2025 06:24:30 - INFO - __main__ -   Step: 210, LR: 5.460829493087558e-05, Loss: 1.5382126569747925
 47%|████▋     | 211/448 [8:24:22<9:10:25, 139.35s/it]02/18/2025 06:26:49 - INFO - __main__ -   Step: 211, LR: 5.4377880184331796e-05, Loss: 1.4460078477859497
 47%|████▋     | 212/448 [8:26:42<9:08:33, 139.47s/it]02/18/2025 06:29:09 - INFO - __main__ -   Step: 212, LR: 5.4147465437788024e-05, Loss: 1.590073823928833
 48%|████▊     | 213/448 [8:29:02<9:07:52, 139.88s/it]02/18/2025 06:31:30 - INFO - __main__ -   Step: 213, LR: 5.3917050691244244e-05, Loss: 1.5768892765045166
 48%|████▊     | 214/448 [8:31:22<9:05:41, 139.92s/it]02/18/2025 06:33:50 - INFO - __main__ -   Step: 214, LR: 5.368663594470046e-05, Loss: 1.5613820552825928
 48%|████▊     | 215/448 [8:33:42<9:03:20, 139.92s/it]02/18/2025 06:36:10 - INFO - __main__ -   Step: 215, LR: 5.3456221198156686e-05, Loss: 1.4714378118515015
 48%|████▊     | 216/448 [8:36:04<9:03:30, 140.56s/it]02/18/2025 06:38:32 - INFO - __main__ -   Step: 216, LR: 5.32258064516129e-05, Loss: 1.5359948873519897
 48%|████▊     | 217/448 [8:38:24<8:59:27, 140.12s/it]02/18/2025 06:40:51 - INFO - __main__ -   Step: 217, LR: 5.2995391705069134e-05, Loss: 1.4962091445922852
 49%|████▊     | 218/448 [8:40:41<8:54:21, 139.40s/it]02/18/2025 06:43:08 - INFO - __main__ -   Step: 218, LR: 5.276497695852535e-05, Loss: 1.5127030611038208
 49%|████▉     | 219/448 [8:43:05<8:57:25, 140.81s/it]02/18/2025 06:45:32 - INFO - __main__ -   Step: 219, LR: 5.253456221198156e-05, Loss: 1.5959159135818481
 49%|████▉     | 220/448 [8:45:27<8:55:53, 141.02s/it]02/18/2025 06:47:54 - INFO - __main__ -   Step: 220, LR: 5.230414746543779e-05, Loss: 1.531253695487976
 49%|████▉     | 221/448 [8:47:54<9:00:11, 142.78s/it]02/18/2025 06:50:21 - INFO - __main__ -   Step: 221, LR: 5.207373271889401e-05, Loss: 1.568068265914917
 50%|████▉     | 222/448 [8:50:11<8:51:55, 141.22s/it]02/18/2025 06:52:38 - INFO - __main__ -   Step: 222, LR: 5.184331797235024e-05, Loss: 1.5790472030639648
 50%|████▉     | 223/448 [8:52:31<8:48:01, 140.80s/it]02/18/2025 06:54:58 - INFO - __main__ -   Step: 223, LR: 5.161290322580645e-05, Loss: 1.5043799877166748
loading configuration file config.json from cache at /home/wangyatong/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json
Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32000
}

 50%|█████     | 224/448 [8:55:48<9:48:54, 157.75s/it]02/18/2025 06:58:16 - INFO - __main__ -   Step: 224, LR: 5.138248847926268e-05, Loss: 1.5194758176803589
 50%|█████     | 225/448 [8:58:08<9:25:46, 152.23s/it]02/18/2025 07:00:35 - INFO - __main__ -   Step: 225, LR: 5.11520737327189e-05, Loss: 1.5112676620483398
 50%|█████     | 226/448 [9:00:26<9:07:41, 148.03s/it]02/18/2025 07:02:53 - INFO - __main__ -   Step: 226, LR: 5.092165898617511e-05, Loss: 1.5519464015960693
 51%|█████     | 227/448 [9:02:45<8:55:34, 145.41s/it]02/18/2025 07:05:12 - INFO - __main__ -   Step: 227, LR: 5.069124423963134e-05, Loss: 1.5266063213348389
 51%|█████     | 228/448 [9:05:06<8:47:59, 144.00s/it]02/18/2025 07:07:33 - INFO - __main__ -   Step: 228, LR: 5.046082949308756e-05, Loss: 1.580171823501587
 51%|█████     | 229/448 [9:07:26<8:41:20, 142.83s/it]02/18/2025 07:09:53 - INFO - __main__ -   Step: 229, LR: 5.023041474654379e-05, Loss: 1.5453557968139648
 51%|█████▏    | 230/448 [9:09:45<8:34:54, 141.72s/it]02/18/2025 07:12:12 - INFO - __main__ -   Step: 230, LR: 5e-05, Loss: 1.4677484035491943
 52%|█████▏    | 231/448 [9:12:02<8:27:25, 140.30s/it]02/18/2025 07:14:29 - INFO - __main__ -   Step: 231, LR: 4.976958525345622e-05, Loss: 1.5196524858474731
 52%|█████▏    | 232/448 [9:14:23<8:25:52, 140.52s/it]02/18/2025 07:16:50 - INFO - __main__ -   Step: 232, LR: 4.9539170506912444e-05, Loss: 1.4961374998092651
 52%|█████▏    | 233/448 [9:16:42<8:21:44, 140.02s/it]02/18/2025 07:19:09 - INFO - __main__ -   Step: 233, LR: 4.9308755760368664e-05, Loss: 1.5300973653793335
 52%|█████▏    | 234/448 [9:19:01<8:17:54, 139.60s/it]02/18/2025 07:21:28 - INFO - __main__ -   Step: 234, LR: 4.9078341013824885e-05, Loss: 1.5265545845031738
 52%|█████▏    | 235/448 [9:21:19<8:14:15, 139.23s/it]02/18/2025 07:23:46 - INFO - __main__ -   Step: 235, LR: 4.8847926267281106e-05, Loss: 1.5341360569000244
 53%|█████▎    | 236/448 [9:23:39<8:12:26, 139.37s/it]02/18/2025 07:26:06 - INFO - __main__ -   Step: 236, LR: 4.861751152073733e-05, Loss: 1.4750683307647705
 53%|█████▎    | 237/448 [9:26:00<8:11:35, 139.79s/it]02/18/2025 07:28:27 - INFO - __main__ -   Step: 237, LR: 4.8387096774193554e-05, Loss: 1.4869968891143799
 53%|█████▎    | 238/448 [9:28:19<8:09:12, 139.78s/it]02/18/2025 07:30:46 - INFO - __main__ -   Step: 238, LR: 4.8156682027649774e-05, Loss: 1.4672327041625977
 53%|█████▎    | 239/448 [9:30:42<8:10:18, 140.76s/it]02/18/2025 07:33:09 - INFO - __main__ -   Step: 239, LR: 4.792626728110599e-05, Loss: 1.4735143184661865
 54%|█████▎    | 240/448 [9:32:59<8:04:10, 139.67s/it]02/18/2025 07:35:27 - INFO - __main__ -   Step: 240, LR: 4.7695852534562216e-05, Loss: 1.544670820236206
 54%|█████▍    | 241/448 [9:35:20<8:02:35, 139.88s/it]02/18/2025 07:37:47 - INFO - __main__ -   Step: 241, LR: 4.7465437788018436e-05, Loss: 1.531203031539917
 54%|█████▍    | 242/448 [9:37:38<7:58:12, 139.28s/it]02/18/2025 07:40:05 - INFO - __main__ -   Step: 242, LR: 4.723502304147466e-05, Loss: 1.4960992336273193
 54%|█████▍    | 243/448 [9:39:56<7:54:49, 138.97s/it]02/18/2025 07:42:23 - INFO - __main__ -   Step: 243, LR: 4.700460829493088e-05, Loss: 1.4838378429412842
 54%|█████▍    | 244/448 [9:42:16<7:53:48, 139.35s/it]02/18/2025 07:44:43 - INFO - __main__ -   Step: 244, LR: 4.67741935483871e-05, Loss: 1.5384514331817627
 55%|█████▍    | 245/448 [9:44:34<7:49:28, 138.76s/it]02/18/2025 07:47:01 - INFO - __main__ -   Step: 245, LR: 4.654377880184332e-05, Loss: 1.4851858615875244
 55%|█████▍    | 246/448 [9:46:52<7:46:27, 138.55s/it]02/18/2025 07:49:19 - INFO - __main__ -   Step: 246, LR: 4.631336405529954e-05, Loss: 1.4795408248901367
 55%|█████▌    | 247/448 [9:49:10<7:44:09, 138.55s/it]02/18/2025 07:51:37 - INFO - __main__ -   Step: 247, LR: 4.608294930875576e-05, Loss: 1.5460481643676758
 55%|█████▌    | 248/448 [9:51:33<7:45:48, 139.74s/it]02/18/2025 07:54:00 - INFO - __main__ -   Step: 248, LR: 4.585253456221199e-05, Loss: 1.5209470987319946
 56%|█████▌    | 249/448 [9:53:50<7:40:59, 138.99s/it]02/18/2025 07:56:17 - INFO - __main__ -   Step: 249, LR: 4.562211981566821e-05, Loss: 1.5077135562896729
 56%|█████▌    | 250/448 [9:56:13<7:42:16, 140.09s/it]02/18/2025 07:58:40 - INFO - __main__ -   Step: 250, LR: 4.539170506912442e-05, Loss: 1.4864312410354614
 56%|█████▌    | 251/448 [9:58:34<7:41:31, 140.57s/it]02/18/2025 08:01:01 - INFO - __main__ -   Step: 251, LR: 4.516129032258064e-05, Loss: 1.501551866531372
 56%|█████▋    | 252/448 [10:00:54<7:38:32, 140.37s/it]02/18/2025 08:03:21 - INFO - __main__ -   Step: 252, LR: 4.493087557603687e-05, Loss: 1.4888255596160889
 56%|█████▋    | 253/448 [10:03:14<7:35:14, 140.07s/it]02/18/2025 08:05:41 - INFO - __main__ -   Step: 253, LR: 4.470046082949309e-05, Loss: 1.5201616287231445
 57%|█████▋    | 254/448 [10:05:33<7:32:32, 139.96s/it]02/18/2025 08:08:00 - INFO - __main__ -   Step: 254, LR: 4.447004608294931e-05, Loss: 1.532930612564087
 57%|█████▋    | 255/448 [10:07:57<7:33:40, 141.04s/it]02/18/2025 08:10:24 - INFO - __main__ -   Step: 255, LR: 4.423963133640553e-05, Loss: 1.5190669298171997
 57%|█████▋    | 256/448 [10:10:16<7:29:09, 140.36s/it]02/18/2025 08:12:43 - INFO - __main__ -   Step: 256, LR: 4.400921658986175e-05, Loss: 1.5638731718063354
 57%|█████▋    | 257/448 [10:12:33<7:24:02, 139.49s/it]02/18/2025 08:15:00 - INFO - __main__ -   Step: 257, LR: 4.3778801843317974e-05, Loss: 1.5176560878753662
 58%|█████▊    | 258/448 [10:14:51<7:20:08, 138.99s/it]02/18/2025 08:17:18 - INFO - __main__ -   Step: 258, LR: 4.3548387096774194e-05, Loss: 1.4623041152954102
 58%|█████▊    | 259/448 [10:17:09<7:16:48, 138.67s/it]02/18/2025 08:19:36 - INFO - __main__ -   Step: 259, LR: 4.3317972350230415e-05, Loss: 1.47967529296875
 58%|█████▊    | 260/448 [10:19:28<7:14:34, 138.70s/it]02/18/2025 08:21:55 - INFO - __main__ -   Step: 260, LR: 4.308755760368664e-05, Loss: 1.475144386291504
 58%|█████▊    | 261/448 [10:21:45<7:11:08, 138.34s/it]02/18/2025 08:24:12 - INFO - __main__ -   Step: 261, LR: 4.2857142857142856e-05, Loss: 1.4592337608337402
 58%|█████▊    | 262/448 [10:24:04<7:08:57, 138.38s/it]02/18/2025 08:26:31 - INFO - __main__ -   Step: 262, LR: 4.262672811059908e-05, Loss: 1.510545253753662
 59%|█████▊    | 263/448 [10:26:24<7:08:08, 138.86s/it]02/18/2025 08:28:51 - INFO - __main__ -   Step: 263, LR: 4.2396313364055304e-05, Loss: 1.510955810546875
 59%|█████▉    | 264/448 [10:28:42<7:05:05, 138.62s/it]02/18/2025 08:31:09 - INFO - __main__ -   Step: 264, LR: 4.2165898617511525e-05, Loss: 1.5704549551010132
 59%|█████▉    | 265/448 [10:31:03<7:05:38, 139.56s/it]02/18/2025 08:33:30 - INFO - __main__ -   Step: 265, LR: 4.1935483870967746e-05, Loss: 1.4856622219085693
 59%|█████▉    | 266/448 [10:33:23<7:03:42, 139.68s/it]02/18/2025 08:35:50 - INFO - __main__ -   Step: 266, LR: 4.1705069124423966e-05, Loss: 1.4887828826904297
 60%|█████▉    | 267/448 [10:35:43<7:01:38, 139.77s/it]02/18/2025 08:38:10 - INFO - __main__ -   Step: 267, LR: 4.147465437788019e-05, Loss: 1.5233840942382812
 60%|█████▉    | 268/448 [10:38:06<7:01:45, 140.59s/it]02/18/2025 08:40:33 - INFO - __main__ -   Step: 268, LR: 4.124423963133641e-05, Loss: 1.4874932765960693
 60%|██████    | 269/448 [10:40:23<6:56:13, 139.51s/it]02/18/2025 08:42:50 - INFO - __main__ -   Step: 269, LR: 4.101382488479263e-05, Loss: 1.5122230052947998
 60%|██████    | 270/448 [10:42:41<6:52:27, 139.03s/it]02/18/2025 08:45:08 - INFO - __main__ -   Step: 270, LR: 4.078341013824885e-05, Loss: 1.5650298595428467
 60%|██████    | 271/448 [10:44:57<6:47:56, 138.28s/it]02/18/2025 08:47:24 - INFO - __main__ -   Step: 271, LR: 4.0552995391705076e-05, Loss: 1.4710346460342407
 61%|██████    | 272/448 [10:47:17<6:46:52, 138.71s/it]02/18/2025 08:49:44 - INFO - __main__ -   Step: 272, LR: 4.032258064516129e-05, Loss: 1.4823250770568848
 61%|██████    | 273/448 [10:49:37<6:46:08, 139.25s/it]02/18/2025 08:52:05 - INFO - __main__ -   Step: 273, LR: 4.009216589861751e-05, Loss: 1.5069162845611572
 61%|██████    | 274/448 [10:52:01<6:47:36, 140.55s/it]02/18/2025 08:54:28 - INFO - __main__ -   Step: 274, LR: 3.986175115207373e-05, Loss: 1.456399917602539
 61%|██████▏   | 275/448 [10:54:24<6:47:39, 141.38s/it]02/18/2025 08:56:52 - INFO - __main__ -   Step: 275, LR: 3.963133640552996e-05, Loss: 1.4991180896759033
 62%|██████▏   | 276/448 [10:56:44<6:43:28, 140.75s/it]02/18/2025 08:59:11 - INFO - __main__ -   Step: 276, LR: 3.940092165898618e-05, Loss: 1.5251827239990234
 62%|██████▏   | 277/448 [10:59:03<6:39:58, 140.34s/it]02/18/2025 09:01:30 - INFO - __main__ -   Step: 277, LR: 3.91705069124424e-05, Loss: 1.6022279262542725
 62%|██████▏   | 278/448 [11:01:23<6:37:19, 140.23s/it]02/18/2025 09:03:50 - INFO - __main__ -   Step: 278, LR: 3.8940092165898614e-05, Loss: 1.4447394609451294
 62%|██████▏   | 279/448 [11:03:41<6:32:46, 139.45s/it]02/18/2025 09:06:08 - INFO - __main__ -   Step: 279, LR: 3.870967741935484e-05, Loss: 1.4495937824249268
 62%|██████▎   | 280/448 [11:05:59<6:29:31, 139.11s/it]02/18/2025 09:08:26 - INFO - __main__ -   Step: 280, LR: 3.847926267281106e-05, Loss: 1.561502456665039
 63%|██████▎   | 281/448 [11:08:19<6:28:02, 139.42s/it]02/18/2025 09:10:46 - INFO - __main__ -   Step: 281, LR: 3.824884792626728e-05, Loss: 1.520973563194275
 63%|██████▎   | 282/448 [11:10:38<6:25:32, 139.35s/it]02/18/2025 09:13:05 - INFO - __main__ -   Step: 282, LR: 3.8018433179723504e-05, Loss: 1.5171887874603271
 63%|██████▎   | 283/448 [11:13:00<6:24:51, 139.95s/it]02/18/2025 09:15:27 - INFO - __main__ -   Step: 283, LR: 3.7788018433179724e-05, Loss: 1.5481305122375488
 63%|██████▎   | 284/448 [11:15:19<6:22:06, 139.80s/it]02/18/2025 09:17:46 - INFO - __main__ -   Step: 284, LR: 3.7557603686635945e-05, Loss: 1.5003488063812256
 64%|██████▎   | 285/448 [11:17:38<6:19:00, 139.51s/it]02/18/2025 09:20:05 - INFO - __main__ -   Step: 285, LR: 3.7327188940092166e-05, Loss: 1.498934268951416
 64%|██████▍   | 286/448 [11:19:58<6:17:12, 139.70s/it]02/18/2025 09:22:25 - INFO - __main__ -   Step: 286, LR: 3.7096774193548386e-05, Loss: 1.5002704858779907
 64%|██████▍   | 287/448 [11:22:17<6:14:22, 139.52s/it]02/18/2025 09:24:44 - INFO - __main__ -   Step: 287, LR: 3.6866359447004614e-05, Loss: 1.5627297163009644
 64%|██████▍   | 288/448 [11:24:36<6:11:14, 139.21s/it]02/18/2025 09:27:03 - INFO - __main__ -   Step: 288, LR: 3.6635944700460834e-05, Loss: 1.5365068912506104
 65%|██████▍   | 289/448 [11:26:54<6:07:56, 138.85s/it]02/18/2025 09:29:21 - INFO - __main__ -   Step: 289, LR: 3.640552995391705e-05, Loss: 1.5221248865127563
 65%|██████▍   | 290/448 [11:29:11<6:04:28, 138.41s/it]02/18/2025 09:31:38 - INFO - __main__ -   Step: 290, LR: 3.6175115207373276e-05, Loss: 1.4823362827301025
 65%|██████▍   | 291/448 [11:31:30<6:02:50, 138.67s/it]02/18/2025 09:33:57 - INFO - __main__ -   Step: 291, LR: 3.5944700460829496e-05, Loss: 1.4799237251281738
 65%|██████▌   | 292/448 [11:33:49<6:00:47, 138.77s/it]02/18/2025 09:36:16 - INFO - __main__ -   Step: 292, LR: 3.571428571428572e-05, Loss: 1.5303540229797363
 65%|██████▌   | 293/448 [11:36:09<5:59:22, 139.11s/it]02/18/2025 09:38:36 - INFO - __main__ -   Step: 293, LR: 3.548387096774194e-05, Loss: 1.4912410974502563
 66%|██████▌   | 294/448 [11:38:29<5:57:49, 139.41s/it]02/18/2025 09:40:56 - INFO - __main__ -   Step: 294, LR: 3.525345622119816e-05, Loss: 1.410102128982544
 66%|██████▌   | 295/448 [11:40:47<5:54:04, 138.85s/it]02/18/2025 09:43:14 - INFO - __main__ -   Step: 295, LR: 3.502304147465438e-05, Loss: 1.5016629695892334
 66%|██████▌   | 296/448 [11:43:06<5:52:17, 139.06s/it]02/18/2025 09:45:34 - INFO - __main__ -   Step: 296, LR: 3.47926267281106e-05, Loss: 1.4681109189987183
 66%|██████▋   | 297/448 [11:45:27<5:51:01, 139.48s/it]02/18/2025 09:47:54 - INFO - __main__ -   Step: 297, LR: 3.456221198156682e-05, Loss: 1.4705095291137695
 67%|██████▋   | 298/448 [11:47:45<5:47:18, 138.92s/it]02/18/2025 09:50:12 - INFO - __main__ -   Step: 298, LR: 3.433179723502305e-05, Loss: 1.5299285650253296
 67%|██████▋   | 299/448 [11:50:02<5:44:08, 138.58s/it]02/18/2025 09:52:29 - INFO - __main__ -   Step: 299, LR: 3.410138248847927e-05, Loss: 1.590056300163269
 67%|██████▋   | 300/448 [11:52:18<5:39:46, 137.75s/it]02/18/2025 09:54:45 - INFO - __main__ -   Step: 300, LR: 3.387096774193548e-05, Loss: 1.4977638721466064
 67%|██████▋   | 301/448 [11:54:39<5:40:09, 138.84s/it]02/18/2025 09:57:07 - INFO - __main__ -   Step: 301, LR: 3.36405529953917e-05, Loss: 1.5103739500045776
 67%|██████▋   | 302/448 [11:57:01<5:39:29, 139.52s/it]02/18/2025 09:59:28 - INFO - __main__ -   Step: 302, LR: 3.341013824884793e-05, Loss: 1.4878168106079102
 68%|██████▊   | 303/448 [11:59:19<5:36:24, 139.20s/it]02/18/2025 10:01:46 - INFO - __main__ -   Step: 303, LR: 3.317972350230415e-05, Loss: 1.4913711547851562
 68%|██████▊   | 304/448 [12:01:37<5:32:51, 138.69s/it]02/18/2025 10:04:04 - INFO - __main__ -   Step: 304, LR: 3.294930875576037e-05, Loss: 1.5048624277114868
 68%|██████▊   | 305/448 [12:03:54<5:29:47, 138.37s/it]02/18/2025 10:06:21 - INFO - __main__ -   Step: 305, LR: 3.271889400921659e-05, Loss: 1.4564309120178223
 68%|██████▊   | 306/448 [12:06:16<5:30:10, 139.51s/it]02/18/2025 10:08:43 - INFO - __main__ -   Step: 306, LR: 3.248847926267281e-05, Loss: 1.4856570959091187
 69%|██████▊   | 307/448 [12:08:37<5:28:25, 139.75s/it]02/18/2025 10:11:04 - INFO - __main__ -   Step: 307, LR: 3.2258064516129034e-05, Loss: 1.4971952438354492
 69%|██████▉   | 308/448 [12:10:56<5:25:29, 139.50s/it]02/18/2025 10:13:23 - INFO - __main__ -   Step: 308, LR: 3.2027649769585254e-05, Loss: 1.506179928779602
 69%|██████▉   | 309/448 [12:13:15<5:23:14, 139.53s/it]02/18/2025 10:15:42 - INFO - __main__ -   Step: 309, LR: 3.1797235023041475e-05, Loss: 1.4580187797546387
 69%|██████▉   | 310/448 [12:15:31<5:18:32, 138.49s/it]02/18/2025 10:17:58 - INFO - __main__ -   Step: 310, LR: 3.15668202764977e-05, Loss: 1.5153471231460571
 69%|██████▉   | 311/448 [12:17:53<5:18:45, 139.60s/it]02/18/2025 10:20:21 - INFO - __main__ -   Step: 311, LR: 3.1336405529953916e-05, Loss: 1.4848854541778564
 70%|██████▉   | 312/448 [12:20:14<5:17:09, 139.92s/it]02/18/2025 10:22:41 - INFO - __main__ -   Step: 312, LR: 3.110599078341014e-05, Loss: 1.479996919631958
 70%|██████▉   | 313/448 [12:22:34<5:15:05, 140.04s/it]02/18/2025 10:25:02 - INFO - __main__ -   Step: 313, LR: 3.087557603686636e-05, Loss: 1.5075095891952515
 70%|███████   | 314/448 [12:24:53<5:11:50, 139.63s/it]02/18/2025 10:27:20 - INFO - __main__ -   Step: 314, LR: 3.0645161290322585e-05, Loss: 1.5246442556381226
 70%|███████   | 315/448 [12:27:12<5:09:15, 139.51s/it]02/18/2025 10:29:39 - INFO - __main__ -   Step: 315, LR: 3.0414746543778806e-05, Loss: 1.4798274040222168
 71%|███████   | 316/448 [12:29:30<5:05:47, 139.00s/it]02/18/2025 10:31:57 - INFO - __main__ -   Step: 316, LR: 3.0184331797235026e-05, Loss: 1.501153826713562
 71%|███████   | 317/448 [12:31:50<5:04:08, 139.30s/it]02/18/2025 10:34:17 - INFO - __main__ -   Step: 317, LR: 2.9953917050691244e-05, Loss: 1.5198785066604614
 71%|███████   | 318/448 [12:34:11<5:02:35, 139.66s/it]02/18/2025 10:36:38 - INFO - __main__ -   Step: 318, LR: 2.9723502304147464e-05, Loss: 1.5216611623764038
 71%|███████   | 319/448 [12:36:30<5:00:11, 139.62s/it]02/18/2025 10:38:57 - INFO - __main__ -   Step: 319, LR: 2.9493087557603688e-05, Loss: 1.5119783878326416
 71%|███████▏  | 320/448 [12:38:50<4:57:41, 139.54s/it]02/18/2025 10:41:17 - INFO - __main__ -   Step: 320, LR: 2.926267281105991e-05, Loss: 1.5518629550933838
 72%|███████▏  | 321/448 [12:41:09<4:55:19, 139.52s/it]02/18/2025 10:43:36 - INFO - __main__ -   Step: 321, LR: 2.9032258064516133e-05, Loss: 1.4454361200332642
 72%|███████▏  | 322/448 [12:43:26<4:51:13, 138.68s/it]02/18/2025 10:45:53 - INFO - __main__ -   Step: 322, LR: 2.880184331797235e-05, Loss: 1.4675323963165283
 72%|███████▏  | 323/448 [12:45:43<4:47:59, 138.24s/it]02/18/2025 10:48:10 - INFO - __main__ -   Step: 323, LR: 2.857142857142857e-05, Loss: 1.5259735584259033
 72%|███████▏  | 324/448 [12:48:04<4:47:36, 139.17s/it]02/18/2025 10:50:31 - INFO - __main__ -   Step: 324, LR: 2.8341013824884795e-05, Loss: 1.50564706325531
 73%|███████▎  | 325/448 [12:50:23<4:45:13, 139.14s/it]02/18/2025 10:52:50 - INFO - __main__ -   Step: 325, LR: 2.8110599078341016e-05, Loss: 1.498443841934204
 73%|███████▎  | 326/448 [12:52:41<4:42:17, 138.83s/it]02/18/2025 10:55:09 - INFO - __main__ -   Step: 326, LR: 2.7880184331797236e-05, Loss: 1.4855401515960693
 73%|███████▎  | 327/448 [12:55:00<4:39:41, 138.69s/it]02/18/2025 10:57:27 - INFO - __main__ -   Step: 327, LR: 2.764976958525346e-05, Loss: 1.5112850666046143
 73%|███████▎  | 328/448 [12:57:18<4:37:09, 138.58s/it]02/18/2025 10:59:45 - INFO - __main__ -   Step: 328, LR: 2.7419354838709678e-05, Loss: 1.475635051727295
 73%|███████▎  | 329/448 [12:59:37<4:34:58, 138.64s/it]02/18/2025 11:02:04 - INFO - __main__ -   Step: 329, LR: 2.7188940092165898e-05, Loss: 1.4826359748840332
 74%|███████▎  | 330/448 [13:01:52<4:30:51, 137.73s/it]02/18/2025 11:04:20 - INFO - __main__ -   Step: 330, LR: 2.6958525345622122e-05, Loss: 1.476775884628296
 74%|███████▍  | 331/448 [13:04:11<4:29:05, 137.99s/it]02/18/2025 11:06:38 - INFO - __main__ -   Step: 331, LR: 2.6728110599078343e-05, Loss: 1.485097885131836
 74%|███████▍  | 332/448 [13:06:32<4:28:11, 138.72s/it]02/18/2025 11:08:59 - INFO - __main__ -   Step: 332, LR: 2.6497695852534567e-05, Loss: 1.4901732206344604
 74%|███████▍  | 333/448 [13:08:51<4:26:30, 139.05s/it]02/18/2025 11:11:18 - INFO - __main__ -   Step: 333, LR: 2.626728110599078e-05, Loss: 1.4731683731079102
 75%|███████▍  | 334/448 [13:11:12<4:25:17, 139.63s/it]02/18/2025 11:13:39 - INFO - __main__ -   Step: 334, LR: 2.6036866359447005e-05, Loss: 1.5165636539459229
 75%|███████▍  | 335/448 [13:13:34<4:23:52, 140.11s/it]02/18/2025 11:16:01 - INFO - __main__ -   Step: 335, LR: 2.5806451612903226e-05, Loss: 1.4929494857788086
 75%|███████▌  | 336/448 [13:15:54<4:21:37, 140.16s/it]02/18/2025 11:18:21 - INFO - __main__ -   Step: 336, LR: 2.557603686635945e-05, Loss: 1.4653968811035156
 75%|███████▌  | 337/448 [13:18:14<4:19:26, 140.24s/it]02/18/2025 11:20:41 - INFO - __main__ -   Step: 337, LR: 2.534562211981567e-05, Loss: 1.4655063152313232
 75%|███████▌  | 338/448 [13:20:33<4:16:03, 139.67s/it]02/18/2025 11:23:00 - INFO - __main__ -   Step: 338, LR: 2.5115207373271894e-05, Loss: 1.490257740020752
 76%|███████▌  | 339/448 [13:22:51<4:12:48, 139.16s/it]02/18/2025 11:25:18 - INFO - __main__ -   Step: 339, LR: 2.488479262672811e-05, Loss: 1.5345696210861206
 76%|███████▌  | 340/448 [13:25:12<4:11:54, 139.95s/it]02/18/2025 11:27:40 - INFO - __main__ -   Step: 340, LR: 2.4654377880184332e-05, Loss: 1.4576997756958008
 76%|███████▌  | 341/448 [13:27:32<4:09:16, 139.78s/it]02/18/2025 11:29:59 - INFO - __main__ -   Step: 341, LR: 2.4423963133640553e-05, Loss: 1.5099838972091675
 76%|███████▋  | 342/448 [13:29:52<4:07:04, 139.85s/it]02/18/2025 11:32:19 - INFO - __main__ -   Step: 342, LR: 2.4193548387096777e-05, Loss: 1.5014337301254272
 77%|███████▋  | 343/448 [13:32:11<4:04:35, 139.77s/it]02/18/2025 11:34:39 - INFO - __main__ -   Step: 343, LR: 2.3963133640552994e-05, Loss: 1.5144296884536743
 77%|███████▋  | 344/448 [13:34:29<4:01:09, 139.13s/it]02/18/2025 11:36:56 - INFO - __main__ -   Step: 344, LR: 2.3732718894009218e-05, Loss: 1.5538837909698486
 77%|███████▋  | 345/448 [13:36:49<3:59:09, 139.31s/it]02/18/2025 11:39:16 - INFO - __main__ -   Step: 345, LR: 2.350230414746544e-05, Loss: 1.4639195203781128
 77%|███████▋  | 346/448 [13:39:08<3:56:35, 139.18s/it]02/18/2025 11:41:35 - INFO - __main__ -   Step: 346, LR: 2.327188940092166e-05, Loss: 1.4949510097503662
 77%|███████▋  | 347/448 [13:41:28<3:54:42, 139.43s/it]02/18/2025 11:43:55 - INFO - __main__ -   Step: 347, LR: 2.304147465437788e-05, Loss: 1.4984300136566162
 78%|███████▊  | 348/448 [13:43:45<3:51:33, 138.93s/it]02/18/2025 11:46:13 - INFO - __main__ -   Step: 348, LR: 2.2811059907834104e-05, Loss: 1.4750564098358154
 78%|███████▊  | 349/448 [13:46:03<3:48:33, 138.52s/it]02/18/2025 11:48:30 - INFO - __main__ -   Step: 349, LR: 2.258064516129032e-05, Loss: 1.4838248491287231
 78%|███████▊  | 350/448 [13:48:20<3:45:20, 137.97s/it]02/18/2025 11:50:47 - INFO - __main__ -   Step: 350, LR: 2.2350230414746546e-05, Loss: 1.5268205404281616
 78%|███████▊  | 351/448 [13:50:39<3:43:44, 138.40s/it]02/18/2025 11:53:06 - INFO - __main__ -   Step: 351, LR: 2.2119815668202766e-05, Loss: 1.516129970550537
 79%|███████▊  | 352/448 [13:52:59<3:42:09, 138.85s/it]02/18/2025 11:55:26 - INFO - __main__ -   Step: 352, LR: 2.1889400921658987e-05, Loss: 1.5001122951507568
 79%|███████▉  | 353/448 [13:55:17<3:39:26, 138.59s/it]02/18/2025 11:57:44 - INFO - __main__ -   Step: 353, LR: 2.1658986175115207e-05, Loss: 1.4962904453277588
 79%|███████▉  | 354/448 [13:57:36<3:37:08, 138.60s/it]02/18/2025 12:00:03 - INFO - __main__ -   Step: 354, LR: 2.1428571428571428e-05, Loss: 1.5142536163330078
 79%|███████▉  | 355/448 [13:59:54<3:34:58, 138.69s/it]02/18/2025 12:02:22 - INFO - __main__ -   Step: 355, LR: 2.1198156682027652e-05, Loss: 1.4839403629302979
 79%|███████▉  | 356/448 [14:02:12<3:32:08, 138.36s/it]02/18/2025 12:04:39 - INFO - __main__ -   Step: 356, LR: 2.0967741935483873e-05, Loss: 1.541755199432373
 80%|███████▉  | 357/448 [14:04:31<3:30:15, 138.63s/it]02/18/2025 12:06:58 - INFO - __main__ -   Step: 357, LR: 2.0737327188940094e-05, Loss: 1.4905184507369995
 80%|███████▉  | 358/448 [14:06:49<3:27:37, 138.42s/it]02/18/2025 12:09:16 - INFO - __main__ -   Step: 358, LR: 2.0506912442396314e-05, Loss: 1.5477948188781738
 80%|████████  | 359/448 [14:09:05<3:24:17, 137.73s/it]02/18/2025 12:11:32 - INFO - __main__ -   Step: 359, LR: 2.0276497695852538e-05, Loss: 1.5038728713989258
 80%|████████  | 360/448 [14:11:23<3:22:06, 137.80s/it]02/18/2025 12:13:50 - INFO - __main__ -   Step: 360, LR: 2.0046082949308755e-05, Loss: 1.4996494054794312
 81%|████████  | 361/448 [14:13:44<3:21:07, 138.71s/it]02/18/2025 12:16:11 - INFO - __main__ -   Step: 361, LR: 1.981566820276498e-05, Loss: 1.5569076538085938
 81%|████████  | 362/448 [14:16:01<3:18:04, 138.20s/it]02/18/2025 12:18:28 - INFO - __main__ -   Step: 362, LR: 1.95852534562212e-05, Loss: 1.5089950561523438
 81%|████████  | 363/448 [14:18:20<3:15:57, 138.32s/it]02/18/2025 12:20:47 - INFO - __main__ -   Step: 363, LR: 1.935483870967742e-05, Loss: 1.5151365995407104
 81%|████████▏ | 364/448 [14:20:38<3:13:46, 138.41s/it]02/18/2025 12:23:06 - INFO - __main__ -   Step: 364, LR: 1.912442396313364e-05, Loss: 1.5299110412597656
 81%|████████▏ | 365/448 [14:22:56<3:11:14, 138.25s/it]02/18/2025 12:25:23 - INFO - __main__ -   Step: 365, LR: 1.8894009216589862e-05, Loss: 1.494429111480713
 82%|████████▏ | 366/448 [14:25:19<3:10:43, 139.55s/it]02/18/2025 12:27:46 - INFO - __main__ -   Step: 366, LR: 1.8663594470046083e-05, Loss: 1.5196939706802368
 82%|████████▏ | 367/448 [14:27:35<3:07:12, 138.67s/it]02/18/2025 12:30:03 - INFO - __main__ -   Step: 367, LR: 1.8433179723502307e-05, Loss: 1.5132559537887573
 82%|████████▏ | 368/448 [14:29:56<3:05:28, 139.11s/it]02/18/2025 12:32:23 - INFO - __main__ -   Step: 368, LR: 1.8202764976958524e-05, Loss: 1.514998197555542
 82%|████████▏ | 369/448 [14:32:13<3:02:36, 138.69s/it]02/18/2025 12:34:40 - INFO - __main__ -   Step: 369, LR: 1.7972350230414748e-05, Loss: 1.5044081211090088
 83%|████████▎ | 370/448 [14:34:32<3:00:19, 138.72s/it]02/18/2025 12:36:59 - INFO - __main__ -   Step: 370, LR: 1.774193548387097e-05, Loss: 1.443040370941162
 83%|████████▎ | 371/448 [14:36:50<2:57:52, 138.60s/it]02/18/2025 12:39:18 - INFO - __main__ -   Step: 371, LR: 1.751152073732719e-05, Loss: 1.486290693283081
 83%|████████▎ | 372/448 [14:39:07<2:54:52, 138.05s/it]02/18/2025 12:41:34 - INFO - __main__ -   Step: 372, LR: 1.728110599078341e-05, Loss: 1.4664407968521118
 83%|████████▎ | 373/448 [14:41:25<2:52:20, 137.87s/it]02/18/2025 12:43:52 - INFO - __main__ -   Step: 373, LR: 1.7050691244239634e-05, Loss: 1.5001752376556396
 83%|████████▎ | 374/448 [14:43:46<2:51:27, 139.02s/it]02/18/2025 12:46:13 - INFO - __main__ -   Step: 374, LR: 1.682027649769585e-05, Loss: 1.5082924365997314
 84%|████████▎ | 375/448 [14:46:04<2:48:38, 138.60s/it]02/18/2025 12:48:31 - INFO - __main__ -   Step: 375, LR: 1.6589861751152075e-05, Loss: 1.5023407936096191
 84%|████████▍ | 376/448 [14:48:22<2:46:12, 138.50s/it]02/18/2025 12:50:49 - INFO - __main__ -   Step: 376, LR: 1.6359447004608296e-05, Loss: 1.542271375656128
 84%|████████▍ | 377/448 [14:50:43<2:44:38, 139.14s/it]02/18/2025 12:53:10 - INFO - __main__ -   Step: 377, LR: 1.6129032258064517e-05, Loss: 1.5098201036453247
 84%|████████▍ | 378/448 [14:53:01<2:41:59, 138.85s/it]02/18/2025 12:55:28 - INFO - __main__ -   Step: 378, LR: 1.5898617511520737e-05, Loss: 1.525347352027893
 85%|████████▍ | 379/448 [14:55:17<2:38:44, 138.03s/it]02/18/2025 12:57:44 - INFO - __main__ -   Step: 379, LR: 1.5668202764976958e-05, Loss: 1.4764894247055054
 85%|████████▍ | 380/448 [14:57:37<2:36:55, 138.46s/it]02/18/2025 13:00:04 - INFO - __main__ -   Step: 380, LR: 1.543778801843318e-05, Loss: 1.5146743059158325
 85%|████████▌ | 381/448 [14:59:54<2:34:17, 138.17s/it]02/18/2025 13:02:21 - INFO - __main__ -   Step: 381, LR: 1.5207373271889403e-05, Loss: 1.4060609340667725
 85%|████████▌ | 382/448 [15:02:11<2:31:37, 137.84s/it]02/18/2025 13:04:38 - INFO - __main__ -   Step: 382, LR: 1.4976958525345622e-05, Loss: 1.579062819480896
 85%|████████▌ | 383/448 [15:04:28<2:29:05, 137.63s/it]02/18/2025 13:06:55 - INFO - __main__ -   Step: 383, LR: 1.4746543778801844e-05, Loss: 1.5575079917907715
 86%|████████▌ | 384/448 [15:06:46<2:26:40, 137.50s/it]02/18/2025 13:09:13 - INFO - __main__ -   Step: 384, LR: 1.4516129032258066e-05, Loss: 1.442617654800415
 86%|████████▌ | 385/448 [15:09:02<2:23:56, 137.09s/it]02/18/2025 13:11:29 - INFO - __main__ -   Step: 385, LR: 1.4285714285714285e-05, Loss: 1.5083887577056885
 86%|████████▌ | 386/448 [15:11:22<2:22:49, 138.21s/it]02/18/2025 13:13:50 - INFO - __main__ -   Step: 386, LR: 1.4055299539170508e-05, Loss: 1.4927213191986084
 86%|████████▋ | 387/448 [15:13:44<2:21:23, 139.08s/it]02/18/2025 13:16:11 - INFO - __main__ -   Step: 387, LR: 1.382488479262673e-05, Loss: 1.4777437448501587
 87%|████████▋ | 388/448 [15:16:05<2:19:43, 139.73s/it]02/18/2025 13:18:32 - INFO - __main__ -   Step: 388, LR: 1.3594470046082949e-05, Loss: 1.4555071592330933
 87%|████████▋ | 389/448 [15:18:25<2:17:32, 139.87s/it]02/18/2025 13:20:52 - INFO - __main__ -   Step: 389, LR: 1.3364055299539171e-05, Loss: 1.475953459739685
 87%|████████▋ | 390/448 [15:20:43<2:14:44, 139.38s/it]02/18/2025 13:23:10 - INFO - __main__ -   Step: 390, LR: 1.313364055299539e-05, Loss: 1.5036470890045166
 87%|████████▋ | 391/448 [15:23:02<2:12:09, 139.12s/it]02/18/2025 13:25:29 - INFO - __main__ -   Step: 391, LR: 1.2903225806451613e-05, Loss: 1.4167218208312988
 88%|████████▊ | 392/448 [15:25:18<2:09:06, 138.33s/it]02/18/2025 13:27:45 - INFO - __main__ -   Step: 392, LR: 1.2672811059907835e-05, Loss: 1.5005806684494019
 88%|████████▊ | 393/448 [15:27:37<2:06:54, 138.44s/it]02/18/2025 13:30:04 - INFO - __main__ -   Step: 393, LR: 1.2442396313364056e-05, Loss: 1.5035232305526733
 88%|████████▊ | 394/448 [15:29:55<2:04:25, 138.26s/it]02/18/2025 13:32:22 - INFO - __main__ -   Step: 394, LR: 1.2211981566820276e-05, Loss: 1.4637826681137085
 88%|████████▊ | 395/448 [15:32:12<2:01:48, 137.89s/it]02/18/2025 13:34:39 - INFO - __main__ -   Step: 395, LR: 1.1981566820276497e-05, Loss: 1.502671718597412
 88%|████████▊ | 396/448 [15:34:29<1:59:23, 137.77s/it]02/18/2025 13:36:56 - INFO - __main__ -   Step: 396, LR: 1.175115207373272e-05, Loss: 1.4395328760147095
 89%|████████▊ | 397/448 [15:36:50<1:57:50, 138.63s/it]02/18/2025 13:39:17 - INFO - __main__ -   Step: 397, LR: 1.152073732718894e-05, Loss: 1.482126235961914
 89%|████████▉ | 398/448 [15:39:11<1:56:13, 139.47s/it]02/18/2025 13:41:39 - INFO - __main__ -   Step: 398, LR: 1.129032258064516e-05, Loss: 1.4827063083648682
 89%|████████▉ | 399/448 [15:41:28<1:53:13, 138.64s/it]02/18/2025 13:43:55 - INFO - __main__ -   Step: 399, LR: 1.1059907834101383e-05, Loss: 1.455338716506958
 89%|████████▉ | 400/448 [15:43:47<1:50:52, 138.59s/it]02/18/2025 13:46:14 - INFO - __main__ -   Step: 400, LR: 1.0829493087557604e-05, Loss: 1.4762486219406128
 90%|████████▉ | 401/448 [15:46:07<1:48:59, 139.14s/it]02/18/2025 13:48:34 - INFO - __main__ -   Step: 401, LR: 1.0599078341013826e-05, Loss: 1.5142371654510498
 90%|████████▉ | 402/448 [15:48:26<1:46:37, 139.07s/it]02/18/2025 13:50:53 - INFO - __main__ -   Step: 402, LR: 1.0368663594470047e-05, Loss: 1.4906017780303955
 90%|████████▉ | 403/448 [15:50:44<1:44:03, 138.75s/it]02/18/2025 13:53:11 - INFO - __main__ -   Step: 403, LR: 1.0138248847926269e-05, Loss: 1.472501277923584
 90%|█████████ | 404/448 [15:53:07<1:42:43, 140.07s/it]02/18/2025 13:55:34 - INFO - __main__ -   Step: 404, LR: 9.90783410138249e-06, Loss: 1.4683449268341064
 90%|█████████ | 405/448 [15:55:25<1:39:54, 139.41s/it]02/18/2025 13:57:52 - INFO - __main__ -   Step: 405, LR: 9.67741935483871e-06, Loss: 1.4829686880111694
 91%|█████████ | 406/448 [15:57:42<1:37:09, 138.80s/it]02/18/2025 14:00:09 - INFO - __main__ -   Step: 406, LR: 9.447004608294931e-06, Loss: 1.5177550315856934
 91%|█████████ | 407/448 [16:00:04<1:35:20, 139.53s/it]02/18/2025 14:02:31 - INFO - __main__ -   Step: 407, LR: 9.216589861751153e-06, Loss: 1.4601593017578125
 91%|█████████ | 408/448 [16:02:22<1:32:45, 139.13s/it]02/18/2025 14:04:49 - INFO - __main__ -   Step: 408, LR: 8.986175115207374e-06, Loss: 1.469101905822754
 91%|█████████▏| 409/448 [16:04:42<1:30:43, 139.57s/it]02/18/2025 14:07:09 - INFO - __main__ -   Step: 409, LR: 8.755760368663595e-06, Loss: 1.4909918308258057
 92%|█████████▏| 410/448 [16:07:00<1:27:56, 138.86s/it]02/18/2025 14:09:27 - INFO - __main__ -   Step: 410, LR: 8.525345622119817e-06, Loss: 1.497157096862793
 92%|█████████▏| 411/448 [16:09:18<1:25:31, 138.70s/it]02/18/2025 14:11:45 - INFO - __main__ -   Step: 411, LR: 8.294930875576038e-06, Loss: 1.5229778289794922
 92%|█████████▏| 412/448 [16:11:37<1:23:19, 138.87s/it]02/18/2025 14:14:04 - INFO - __main__ -   Step: 412, LR: 8.064516129032258e-06, Loss: 1.4657760858535767
 92%|█████████▏| 413/448 [16:13:56<1:20:58, 138.82s/it]02/18/2025 14:16:23 - INFO - __main__ -   Step: 413, LR: 7.834101382488479e-06, Loss: 1.418022632598877
 92%|█████████▏| 414/448 [16:16:14<1:18:28, 138.49s/it]02/18/2025 14:18:41 - INFO - __main__ -   Step: 414, LR: 7.603686635944701e-06, Loss: 1.4966118335723877
 93%|█████████▎| 415/448 [16:18:34<1:16:27, 139.00s/it]02/18/2025 14:21:01 - INFO - __main__ -   Step: 415, LR: 7.373271889400922e-06, Loss: 1.481393814086914
 93%|█████████▎| 416/448 [16:20:54<1:14:24, 139.51s/it]02/18/2025 14:23:22 - INFO - __main__ -   Step: 416, LR: 7.142857142857143e-06, Loss: 1.544481635093689
 93%|█████████▎| 417/448 [16:23:12<1:11:42, 138.79s/it]02/18/2025 14:25:39 - INFO - __main__ -   Step: 417, LR: 6.912442396313365e-06, Loss: 1.469048261642456
 93%|█████████▎| 418/448 [16:25:32<1:09:39, 139.33s/it]02/18/2025 14:27:59 - INFO - __main__ -   Step: 418, LR: 6.682027649769586e-06, Loss: 1.4833354949951172
 94%|█████████▎| 419/448 [16:27:51<1:07:19, 139.29s/it]02/18/2025 14:30:18 - INFO - __main__ -   Step: 419, LR: 6.451612903225806e-06, Loss: 1.5052834749221802
 94%|█████████▍| 420/448 [16:30:09<1:04:49, 138.92s/it]02/18/2025 14:32:37 - INFO - __main__ -   Step: 420, LR: 6.221198156682028e-06, Loss: 1.4940376281738281
 94%|█████████▍| 421/448 [16:32:26<1:02:12, 138.26s/it]02/18/2025 14:34:53 - INFO - __main__ -   Step: 421, LR: 5.9907834101382485e-06, Loss: 1.4868295192718506
 94%|█████████▍| 422/448 [16:34:47<1:00:11, 138.92s/it]02/18/2025 14:37:14 - INFO - __main__ -   Step: 422, LR: 5.76036866359447e-06, Loss: 1.4689433574676514
 94%|█████████▍| 423/448 [16:37:05<57:46, 138.66s/it]  02/18/2025 14:39:32 - INFO - __main__ -   Step: 423, LR: 5.5299539170506915e-06, Loss: 1.4715243577957153
 95%|█████████▍| 424/448 [16:39:26<55:44, 139.36s/it]02/18/2025 14:41:53 - INFO - __main__ -   Step: 424, LR: 5.299539170506913e-06, Loss: 1.5445024967193604
 95%|█████████▍| 425/448 [16:41:43<53:13, 138.85s/it]02/18/2025 14:44:10 - INFO - __main__ -   Step: 425, LR: 5.0691244239631346e-06, Loss: 1.4598065614700317
 95%|█████████▌| 426/448 [16:44:03<51:01, 139.14s/it]02/18/2025 14:46:30 - INFO - __main__ -   Step: 426, LR: 4.838709677419355e-06, Loss: 1.44523286819458
 95%|█████████▌| 427/448 [16:46:20<48:28, 138.49s/it]02/18/2025 14:48:47 - INFO - __main__ -   Step: 427, LR: 4.608294930875577e-06, Loss: 1.4973242282867432
 96%|█████████▌| 428/448 [16:48:37<45:59, 137.98s/it]02/18/2025 14:51:04 - INFO - __main__ -   Step: 428, LR: 4.377880184331797e-06, Loss: 1.484328031539917
 96%|█████████▌| 429/448 [16:50:55<43:42, 138.03s/it]02/18/2025 14:53:22 - INFO - __main__ -   Step: 429, LR: 4.147465437788019e-06, Loss: 1.4692248106002808
 96%|█████████▌| 430/448 [16:53:16<41:39, 138.88s/it]02/18/2025 14:55:43 - INFO - __main__ -   Step: 430, LR: 3.9170506912442395e-06, Loss: 1.4758433103561401
 96%|█████████▌| 431/448 [16:55:35<39:21, 138.91s/it]02/18/2025 14:58:02 - INFO - __main__ -   Step: 431, LR: 3.686635944700461e-06, Loss: 1.5543910264968872
 96%|█████████▋| 432/448 [16:57:53<37:01, 138.83s/it]02/18/2025 15:00:21 - INFO - __main__ -   Step: 432, LR: 3.4562211981566825e-06, Loss: 1.4838345050811768
 97%|█████████▋| 433/448 [17:00:11<34:36, 138.42s/it]02/18/2025 15:02:38 - INFO - __main__ -   Step: 433, LR: 3.225806451612903e-06, Loss: 1.549955129623413
 97%|█████████▋| 434/448 [17:02:30<32:19, 138.52s/it]02/18/2025 15:04:57 - INFO - __main__ -   Step: 434, LR: 2.9953917050691243e-06, Loss: 1.5703282356262207
 97%|█████████▋| 435/448 [17:04:48<29:58, 138.38s/it]02/18/2025 15:07:15 - INFO - __main__ -   Step: 435, LR: 2.7649769585253458e-06, Loss: 1.495964527130127
 97%|█████████▋| 436/448 [17:07:08<27:48, 139.02s/it]02/18/2025 15:09:35 - INFO - __main__ -   Step: 436, LR: 2.5345622119815673e-06, Loss: 1.5058915615081787
 98%|█████████▊| 437/448 [17:09:28<25:31, 139.21s/it]02/18/2025 15:11:55 - INFO - __main__ -   Step: 437, LR: 2.3041474654377884e-06, Loss: 1.5423285961151123
 98%|█████████▊| 438/448 [17:11:46<23:09, 138.92s/it]02/18/2025 15:14:13 - INFO - __main__ -   Step: 438, LR: 2.0737327188940094e-06, Loss: 1.4719899892807007
 98%|█████████▊| 439/448 [17:14:04<20:48, 138.67s/it]02/18/2025 15:16:31 - INFO - __main__ -   Step: 439, LR: 1.8433179723502305e-06, Loss: 1.536947250366211
 98%|█████████▊| 440/448 [17:16:21<18:25, 138.13s/it]02/18/2025 15:18:48 - INFO - __main__ -   Step: 440, LR: 1.6129032258064516e-06, Loss: 1.4970080852508545
 98%|█████████▊| 441/448 [17:18:40<16:09, 138.48s/it]02/18/2025 15:21:08 - INFO - __main__ -   Step: 441, LR: 1.3824884792626729e-06, Loss: 1.5032683610916138
 99%|█████████▊| 442/448 [17:20:59<13:50, 138.46s/it]02/18/2025 15:23:26 - INFO - __main__ -   Step: 442, LR: 1.1520737327188942e-06, Loss: 1.5452779531478882
 99%|█████████▉| 443/448 [17:23:18<11:33, 138.71s/it]02/18/2025 15:25:45 - INFO - __main__ -   Step: 443, LR: 9.216589861751153e-07, Loss: 1.4641966819763184
 99%|█████████▉| 444/448 [17:25:36<09:14, 138.58s/it]02/18/2025 15:28:04 - INFO - __main__ -   Step: 444, LR: 6.912442396313364e-07, Loss: 1.5131895542144775
 99%|█████████▉| 445/448 [17:27:55<06:55, 138.57s/it]02/18/2025 15:30:22 - INFO - __main__ -   Step: 445, LR: 4.6082949308755763e-07, Loss: 1.451436996459961
100%|█████████▉| 446/448 [17:30:13<04:36, 138.40s/it]02/18/2025 15:32:40 - INFO - __main__ -   Step: 446, LR: 2.3041474654377881e-07, Loss: 1.5040453672409058
loading configuration file config.json from cache at /home/wangyatong/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json
Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32000
}

tokenizer config file saved in models/llama2-realtimeqa-7b/tokenizer_config.json
Special tokens file saved in models/llama2-realtimeqa-7b/special_tokens_map.json
loading configuration file config.json from cache at /home/wangyatong/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json
Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.48.3",
  "use_cache": true,
  "vocab_size": 32000
}

100%|█████████▉| 446/448 [17:31:25<04:42, 141.45s/it]
